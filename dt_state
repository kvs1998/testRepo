# tabs/dt_state_tab.py
import streamlit as st
import pandas as pd
import plotly.express as px
import numpy as np
from datetime import datetime, time # Import for more precise date handling

# Assuming format_seconds_to_readable is in your utils or is a standalone helper
def format_seconds_to_readable(seconds_series, format_type):
    if format_type == "seconds":
        return seconds_series.round(1).astype(str) + "s"
    elif format_type == "minutes":
        return (seconds_series / 60).round(1).astype(str) + "m"
    elif format_type == "hours":
        return (seconds_series / 3600).round(1).astype(str) + "h"
    elif format_type == "days":
        return (seconds_series / 86400).round(1).astype(str) + "d"
    elif format_type == "mixed":
        def mix_format(s):
            if pd.isna(s) or s is None: return "N/A"
            s = float(s)
            if s == 0: return "0s"

            days = int(s // 86400)
            hours = int((s % 86400) // 3600)
            minutes = int((s % 3600) // 60)
            seconds = s % 60

            parts = []
            if days > 0: parts.append(f"{days}d")
            if hours > 0: parts.append(f"{hours}h")
            if minutes > 0: parts.append(f"{minutes}m")
            if seconds > 0 and (not parts or seconds >= 1):
                parts.append(f"{seconds:.1f}s")

            return " ".join(parts) if parts else "0s"
        return seconds_series.apply(mix_format)
    return seconds_series


def render_dt_state_tab(history_df: pd.DataFrame):
    st.header("Dynamic Table Refresh State & History")
    st.write("Analyze the refresh patterns, durations, and outcomes of your dynamic tables over time.")

    if history_df.empty:
        st.info("No refresh history data available. Check data source or collection.", icon="‚ÑπÔ∏è")
        return

    # --- Data Preprocessing ---
    for col in ['REFRESH_START_TIME', 'REFRESH_END_TIME', 'DATA_TIMESTAMP']:
        if col in history_df.columns:
            history_df[f'{col}_DT'] = pd.to_datetime(history_df[col], errors='coerce')

    if 'REFRESH_START_TIME_DT' in history_df.columns and 'REFRESH_END_TIME_DT' in history_df.columns:
        history_df['REFRESH_DURATION_SEC'] = (
            history_df['REFRESH_END_TIME_DT'] - history_df['REFRESH_START_TIME_DT']
        ).dt.total_seconds().fillna(0) # Fill NaNs (for running refreshes) with 0 for duration calculation

    # Ensure numerical row change columns are numeric
    row_change_raw_cols = ['NUMINSERTEDROWS', 'NUMDELETEDROWS', 'NUMCOPIEDROWS', 'NUMADDEDPARTITIONS', 'NUMREMOVEDPARTITIONS']
    for col in row_change_raw_cols:
        if col in history_df.columns:
            history_df[col] = pd.to_numeric(history_df[col], errors='coerce').fillna(0)


    # --- Filters for DT State Tab ---
    st.markdown("---")
    st.subheader("Apply Filters for Refresh History")

    filter_cols_dt_state_row1 = st.columns([1, 1, 1, 1])

    with filter_cols_dt_state_row1[0]:
        all_databases_dt_state = ['All'] + sorted(history_df['DATABASE_NAME'].unique().tolist())
        selected_database_dt_state = st.selectbox(
            "Database:", options=all_databases_dt_state, key="db_filter_dt_state"
        )
    with filter_cols_dt_state_row1[1]:
        temp_df_for_schema_options = history_df.copy()
        if selected_database_dt_state != 'All':
            temp_df_for_schema_options = temp_df_for_schema_options[temp_df_for_schema_options['DATABASE_NAME'] == selected_database_dt_state]

        if not temp_df_for_schema_options.empty:
            schemas_in_db_dt_state = ['All'] + sorted(temp_df_for_schema_options['SCHEMA_NAME'].unique().tolist())
        else:
            schemas_in_db_dt_state = ['All']
            st.info("No schemas found for selected Database.", icon="‚ÑπÔ∏è")

        selected_schema_dt_state = st.selectbox(
            "Schema:", options=schemas_in_db_dt_state, key="schema_filter_dt_state"
        )

    df_for_table_options_state = history_df.copy()
    if selected_database_dt_state != 'All':
        df_for_table_options_state = df_for_table_options_state[df_for_table_options_state['DATABASE_NAME'] == selected_database_dt_state]
    if selected_schema_dt_state != 'All':
        df_for_table_options_state = df_for_table_options_state[df_for_table_options_state['SCHEMA_NAME'] == selected_schema_dt_state]


    with filter_cols_dt_state_row1[2]:
        if not df_for_table_options_state.empty:
            all_tables_dt_state_options = ['All'] + sorted(df_for_table_options_state['TABLE_NAME'].unique().tolist())
            default_tables_dt_state_selected = ['All']
        else:
            all_tables_dt_state_options = ['All']
            default_tables_dt_state_selected = ['All']
            st.info("No tables found for selected DB/Schema.", icon="‚ÑπÔ∏è")
        selected_table_dt_state = st.multiselect(
            "Table(s):", options=all_tables_dt_state_options, default=default_tables_dt_state_selected, key="table_filter_dt_state"
        )

    with filter_cols_dt_state_row1[3]:
        all_states = ['All'] + sorted(history_df['STATE'].unique().tolist())
        selected_state = st.multiselect(
            "Refresh State(s):", options=all_states, default=['All'], key="refresh_state_filter_dt_state"
        )

    filter_cols_dt_state_row2 = st.columns([1, 1])

    with filter_cols_dt_state_row2[0]:
        # Robust date range handling
        min_date = history_df['DATA_TIMESTAMP_DT'].min().date()
        max_date = history_df['DATA_TIMESTAMP_DT'].max().date()
        
        time_range = st.date_input(
            "Select Date Range for History:",
            value=(min_date, max_date), # Default value as min/max dates
            min_value=min_date,
            max_value=max_date,
            key="date_range_dt_state"
        )
        
        # Ensure time_range is a tuple of 2 dates (Streamlit returns tuple or list)
        if isinstance(time_range, (list, tuple)) and len(time_range) == 2:
            start_date_filter = pd.to_datetime(time_range[0]).normalize() # Start of selected day
            end_date_filter = pd.to_datetime(time_range[1]).normalize() + pd.Timedelta(days=1) - pd.Timedelta(microseconds=1) # End of selected day
        elif isinstance(time_range, (pd.Timestamp, datetime.date, datetime)): # Single date selection
            start_date_filter = pd.to_datetime(time_range).normalize()
            end_date_filter = pd.to_datetime(time_range).normalize() + pd.Timedelta(days=1) - pd.Timedelta(microseconds=1)
        else: # Fallback to full range if selection is invalid or empty
            start_date_filter = history_df['DATA_TIMESTAMP_DT'].min()
            end_date_filter = history_df['DATA_TIMESTAMP_DT'].max()


    with filter_cols_dt_state_row2[1]:
        time_format_option = st.radio(
            "Display Durations In:",
            options=["mixed", "seconds", "minutes", "hours", "days"],
            index=0,
            horizontal=True,
            key="duration_format_dt_state"
        )
    st.markdown("---")

    # --- Apply Filters to Main DataFrame ---
    filtered_history_df = history_df.copy()

    if selected_database_dt_state != 'All':
        filtered_history_df = filtered_history_df[filtered_history_df['DATABASE_NAME'] == selected_database_dt_state].copy()
    if selected_schema_dt_state != 'All':
        filtered_history_df = filtered_history_df[filtered_history_df['SCHEMA_NAME'] == selected_schema_dt_state].copy()
    if selected_table_dt_state and 'All' not in selected_table_dt_state:
        filtered_history_df = filtered_history_df[filtered_history_df['TABLE_NAME'].isin(selected_table_dt_state)].copy()
    elif not selected_table_dt_state:
        st.warning("No table(s) selected. Display will be empty.", icon="‚ö†Ô∏è")
        filtered_history_df = pd.DataFrame()

    if selected_state and 'All' not in selected_state:
        filtered_history_df = filtered_history_df[filtered_history_df['STATE'].isin(selected_state)].copy()
    elif not selected_state:
        st.warning("No refresh state(s) selected. Display will be empty.", icon="‚ö†Ô∏è")
        filtered_history_df = pd.DataFrame()

    # Apply date range filter using Timestamps directly
    filtered_history_df = filtered_history_df[
        (filtered_history_df['DATA_TIMESTAMP_DT'] >= start_date_filter) &
        (filtered_history_df['DATA_TIMESTAMP_DT'] <= end_date_filter)
    ].copy()

    if filtered_history_df.empty:
        st.info("No data available based on current filter selections. Please adjust your filters.", icon="‚ÑπÔ∏è")
        return

    # --- Refresh Trends & Distributions ---
    st.subheader("Refresh Trends & Status Over Time")
    chart_cols_state_row1 = st.columns([1, 1])

    with chart_cols_state_row1[0]:
        st.markdown("<p style='font-size:16px;'><b>Daily Refresh Status Trend</b></p>", unsafe_allow_html=True)
        st.write("Count of refreshes by state over the selected time range.")
        if not filtered_history_df.empty:
            status_trend_df = filtered_history_df.groupby([
                pd.Grouper(key='DATA_TIMESTAMP_DT', freq='D'), 'STATE'
            ]).size().unstack(fill_value=0).reset_index() # fill_value=0 handles missing states for a day
            status_trend_df.rename(columns={'DATA_TIMESTAMP_DT': 'Date'}, inplace=True)

            state_order = ['SUCCEEDED', 'FAILED', 'UPSTREAM_FAILED', 'CANCELLED', 'EXECUTING', 'SCHEDULED']
            present_states = [s for s in state_order if s in status_trend_df.columns]

            status_trend_melted = status_trend_df.melt(
                id_vars=['Date'],
                value_vars=present_states,
                var_name='State',
                value_name='Count'
            )

            fig_status_trend = px.bar(
                status_trend_melted,
                x='Date',
                y='Count',
                color='State',
                title='Daily Refresh Status Trend',
                color_discrete_map={
                    'SUCCEEDED': 'green', 'FAILED': 'red', 'UPSTREAM_FAILED': 'darkred',
                    'CANCELLED': 'orange', 'EXECUTING': 'blue', 'SCHEDULED': 'grey'
                }
            )
            fig_status_trend.update_layout(barmode='stack')

            use_log_scale_refresh_status = st.checkbox("Log Scale Y-axis (Refresh Status Trend)", key="log_scale_refresh_status_dt_state")
            if use_log_scale_refresh_status:
                # To prevent blank charts with log scale, ensure all values are > 0.
                # Adding a small constant to 0s helps in visualization without changing the actual value.
                status_trend_melted['Count_Log'] = status_trend_melted['Count'].apply(lambda x: x if x > 0 else 1e-9)
                fig_status_trend.update_yaxes(type='log', title='Count (Log Scale)', rangemode='tozero')
                # Update bar y-values to use the log-adjusted column for plotting
                fig_status_trend.data[0].y = status_trend_melted[status_trend_melted['State'] == present_states[0]]['Count_Log'] # This is simplified, needs to be updated per trace.
                # A better way is to update the `fig_status_trend` creation with `y='Count_Log'` then update traces.
                # More robust: remake the figure or loop through traces.
                # For simplicity, let's update layout type only and rely on Plotly to handle 0s gracefully or hide them.
                # Often, Plotly will just not draw bars for 0, but a log scale might cause errors if range is 0.
                st.info("Logarithmic scale applied to Y-axis.", icon="‚ÑπÔ∏è")

            st.plotly_chart(fig_status_trend, use_container_width=True)
        else:
            st.info("No data for Refresh Status Trend.", icon="‚ÑπÔ∏è")

    with chart_cols_state_row1[1]:
        st.markdown("<p style='font-size:16px;'><b>Refresh Action Distribution</b></p>", unsafe_allow_html=True)
        st.write("Distribution of refresh actions (e.g., INCREMENTAL, FULL, NO_DATA).")
        if 'REFRESH_ACTION' in filtered_history_df.columns and not filtered_history_df.empty:
            action_counts = filtered_history_df['REFRESH_ACTION'].value_counts().reset_index()
            action_counts.columns = ['Action', 'Count']
            fig_action_dist = px.pie(
                action_counts,
                values='Count',
                names='Action',
                title='Refresh Action Distribution',
                hole=0.3
            )
            fig_action_dist.update_traces(textposition='inside', textinfo='percent+label')
            st.plotly_chart(fig_action_dist, use_container_width=True)
        else:
            st.info("No REFRESH_ACTION data available.", icon="‚ÑπÔ∏è")

    st.divider()

    # --- Performance & Efficiency Analysis ---
    st.subheader("Performance & Efficiency Analysis")
    chart_cols_state_row2 = st.columns([1, 1])

    with chart_cols_state_row2[0]: # Refresh Duration Trend (Line Chart with Averages)
        st.markdown("<p style='font-size:16px;'><b>Refresh Duration Trend</b></p>", unsafe_allow_html=True)
        st.write("Average and Median refresh duration over time.")
        if 'REFRESH_DURATION_SEC' in filtered_history_df.columns and not filtered_history_df['REFRESH_DURATION_SEC'].empty:
            # Aggregate daily average and median duration
            duration_trend_df = filtered_history_df.groupby(
                pd.Grouper(key='DATA_TIMESTAMP_DT', freq='D')
            )['REFRESH_DURATION_SEC'].agg(['mean', 'median', lambda x: x.quantile(0.95)]).reset_index()
            duration_trend_df.columns = ['Date', 'Mean Duration', 'Median Duration', 'P95 Duration']

            fig_duration_trend = px.line(
                duration_trend_df,
                x='Date',
                y=['Mean Duration', 'Median Duration', 'P95 Duration'], # Plot multiple lines
                title='Daily Refresh Duration Trend (Mean, Median, P95)',
                labels={'value': 'Duration (seconds)'} # Default label
            )

            use_log_scale_duration_trend = st.checkbox("Log Scale Y-axis (Duration Trend)", key="log_scale_duration_trend_dt_state")
            if use_log_scale_duration_trend:
                fig_duration_trend.update_yaxes(type='log', title='Duration (Log Scale)', rangemode='tozero')
                st.info("Logarithmic scale applied to Y-axis.", icon="‚ÑπÔ∏è")

            st.plotly_chart(fig_duration_trend, use_container_width=True)
        else:
            st.info("No REFRESH_DURATION_SEC data available for trend analysis.", icon="‚ÑπÔ∏è")


    with chart_cols_state_row2[1]: # Refresh Duration vs. Rows Processed (Scatter Plot)
        st.markdown("<p style='font-size:16px;'><b>Refresh Duration vs. Rows Processed</b></p>", unsafe_allow_html=True)
        st.write("Examine if duration correlates with data volume.")
        
        row_change_selectable_cols = {
            'Total Rows Changed': ['NUMINSERTEDROWS', 'NUMDELETEDROWS', 'NUMCOPIEDROWS', 'NUMADDEDPARTITIONS', 'NUMREMOVEDPARTITIONS'],
            'Inserted Rows': ['NUMINSERTEDROWS'],
            'Deleted Rows': ['NUMDELETEDROWS'],
            'Copied Rows': ['NUMCOPIEDROWS'],
            'Added Partitions': ['NUMADDEDPARTITIONS'],
            'Removed Partitions': ['NUMREMOVEDPARTITIONS']
        }
        
        # Check which columns are actually present in the data
        available_row_metrics = {
            label: cols for label, cols in row_change_selectable_cols.items()
            if all(c in filtered_history_df.columns for c in cols)
        }

        if 'REFRESH_DURATION_SEC' in filtered_history_df.columns and available_row_metrics:
            selected_row_metric_for_scatter = st.selectbox(
                "Select X-axis Row Metric:",
                options=list(available_row_metrics.keys()),
                key="scatter_x_axis_metric"
            )

            cols_to_sum = available_row_metrics[selected_row_metric_for_scatter]
            scatter_df = filtered_history_df.copy()
            scatter_df['X_AXIS_METRIC_VALUE'] = scatter_df[cols_to_sum].sum(axis=1)

            # Filter out NaNs for relevant columns before plotting scatter
            scatter_df = scatter_df.dropna(subset=['REFRESH_DURATION_SEC', 'X_AXIS_METRIC_VALUE']).copy()
            
            # Remove points where both X and Y are zero if log scale is used, as they distort the plot
            scatter_df = scatter_df[(scatter_df['REFRESH_DURATION_SEC'] > 0) | (scatter_df['X_AXIS_METRIC_VALUE'] > 0)]

            if not scatter_df.empty:
                fig_duration_vs_rows = px.scatter(
                    scatter_df,
                    x='X_AXIS_METRIC_VALUE',
                    y='REFRESH_DURATION_SEC',
                    color='STATE', # Color by refresh state
                    hover_name='QUALIFIED_NAME', # Show table name on hover
                    hover_data={
                        'REFRESH_ACTION': True,
                        'REFRESH_START_TIME_DT': True,
                        'REFRESH_END_TIME_DT': True,
                        'STATE_MESSAGE': True,
                        'REFRESH_DURATION_SEC': False, # Hide default, will use customdata
                        'X_AXIS_METRIC_VALUE': False # Hide default, will use customdata
                    },
                    title=f'Refresh Duration vs. {selected_row_metric_for_scatter}',
                    labels={
                        'REFRESH_DURATION_SEC': 'Duration (seconds)',
                        'X_AXIS_METRIC_VALUE': selected_row_metric_for_scatter
                    }
                )

                # Format scatter plot hover for duration and x-axis metric
                scatter_df['REFRESH_DURATION_FMT_HOVER'] = format_seconds_to_readable(scatter_df['REFRESH_DURATION_SEC'], time_format_option)
                scatter_df['X_AXIS_METRIC_FMT_HOVER'] = scatter_df['X_AXIS_METRIC_VALUE'].apply(lambda x: f"{int(x):,}") # Format row count with comma

                fig_duration_vs_rows.update_traces(
                    hovertemplate=(
                        '<b>Table: %{hover_name}</b><br>'
                        'State: %{marker.color}<br>'
                        f'{selected_row_metric_for_scatter}: %{{customdata[1]}}<br>' # Corrected customdata index
                        'Duration: %{customdata[0]}<br>' # Corrected customdata index
                        'Action: %{customdata[2]}<br>'
                        'Start: %{customdata[3]}<br>'
                        'End: %{customdata[4]}<br>'
                        'Message: %{customdata[5]}<br>'
                        '<extra></extra>'
                    ),
                    customdata=np.stack((
                        scatter_df['REFRESH_DURATION_FMT_HOVER'],
                        scatter_df['X_AXIS_METRIC_FMT_HOVER'],
                        scatter_df['REFRESH_ACTION'],
                        scatter_df['REFRESH_START_TIME_DT'],
                        scatter_df['REFRESH_END_TIME_DT'],
                        scatter_df['STATE_MESSAGE']
                    ), axis=-1)
                )

                # Log scale checkboxes (repositioned)
                col_scatter_log1, col_scatter_log2 = st.columns(2)
                with col_scatter_log1:
                    log_x_scatter = st.checkbox("Log Scale X-axis (Rows Processed)", key="log_x_scatter_dt_state")
                with col_scatter_log2:
                    log_y_scatter = st.checkbox("Log Scale Y-axis (Duration Scatter)", key="log_y_scatter_dt_state")

                if log_x_scatter:
                    fig_duration_vs_rows.update_xaxes(type='log')
                if log_y_scatter:
                    fig_duration_vs_rows.update_yaxes(type='log')

                if log_x_scatter or log_y_scatter:
                    st.info("Logarithmic scale applied to selected axis/axes.", icon="‚ÑπÔ∏è")

                st.plotly_chart(fig_duration_vs_rows, use_container_width=True)
            else:
                st.info(f"No valid data for Refresh Duration vs. {selected_row_metric_for_scatter} scatter plot after filters.", icon="‚ÑπÔ∏è")
        else:
            st.info("Necessary columns for Refresh Duration vs. Rows Processed analysis not available or no data after filters.", icon="‚ÑπÔ∏è")
    
    st.divider()

    chart_cols_state_row3 = st.columns([1, 1])

    with chart_cols_state_row3[0]: # Refresh Duration Distribution (Histogram)
        st.markdown("<p style='font-size:16px;'><b>Refresh Duration Distribution</b></p>", unsafe_allow_html=True)
        st.write("Distribution of refresh durations.")
        if 'REFRESH_DURATION_SEC' in filtered_history_df.columns and not filtered_history_df['REFRESH_DURATION_SEC'].empty:
            duration_data = filtered_history_df['REFRESH_DURATION_SEC'].dropna()
            
            max_duration = duration_data.max() if not duration_data.empty else 60

            duration_max_limit_sec = st.slider(
                f"Max Duration for Histogram (seconds):",
                min_value=0.0,
                max_value=float(max_duration),
                value=min(float(max_duration), 3600.0), # Default to 1 hour or max if less
                step=10.0,
                format="%.0f s",
                key="duration_hist_limit"
            )

            # Filter data for histogram based on slider limit, and ensure positive for log scale
            duration_data_for_hist = duration_data[duration_data <= duration_max_limit_sec]
            
            # Ensure no zero values if log scale is to be applied
            if (st.session_state.get("log_scale_duration_hist_dt_state", False) and 0 in duration_data_for_hist.values):
                 duration_data_for_hist = duration_data_for_hist.replace(0, 1e-9) # Replace 0 with tiny positive for log scale

            if not duration_data_for_hist.empty:
                fig_duration_hist = px.histogram(
                    duration_data_for_hist,
                    x="REFRESH_DURATION_SEC",
                    nbins=20,
                    title='Refresh Duration Distribution',
                    labels={'REFRESH_DURATION_SEC': 'Duration'}, # Changed label to be general
                    # Use a custom tickformat for x-axis to show formatted values
                    text_auto=False # Don't show text on bars by default
                )
                fig_duration_hist.update_layout(bargap=0.1)

                use_log_scale_duration_hist = st.checkbox("Log Scale X-axis (Duration Histogram)", key="log_scale_duration_hist_dt_state")
                if use_log_scale_duration_hist:
                    fig_duration_hist.update_xaxes(type='log')
                    st.info("Logarithmic scale applied to X-axis.", icon="‚ÑπÔ∏è")
                
                # Dynamic x-axis tick formatting based on selected time format
                if time_format_option == 'seconds':
                    fig_duration_hist.update_xaxes(tickformat=".1f", ticksuffix="s")
                elif time_format_option == 'minutes':
                    # Manually update tickvals/ticktext for minutes
                    max_x_val = fig_duration_hist.layout.xaxis.range[1] if fig_duration_hist.layout.xaxis.range else duration_data_for_hist.max()
                    tickvals = np.linspace(0, max_x_val, 5) # Example, adjust density as needed
                    ticktext = format_seconds_to_readable(pd.Series(tickvals), 'minutes').tolist()
                    fig_duration_hist.update_xaxes(tickvals=tickvals, ticktext=ticktext)
                elif time_format_option == 'hours':
                     max_x_val = fig_duration_hist.layout.xaxis.range[1] if fig_duration_hist.layout.xaxis.range else duration_data_for_hist.max()
                     tickvals = np.linspace(0, max_x_val, 5)
                     ticktext = format_seconds_to_readable(pd.Series(tickvals), 'hours').tolist()
                     fig_duration_hist.update_xaxes(tickvals=tickvals, ticktext=ticktext)
                elif time_format_option == 'days':
                     max_x_val = fig_duration_hist.layout.xaxis.range[1] if fig_duration_hist.layout.xaxis.range else duration_data_for_hist.max()
                     tickvals = np.linspace(0, max_x_val, 5)
                     ticktext = format_seconds_to_readable(pd.Series(tickvals), 'days').tolist()
                     fig_duration_hist.update_xaxes(tickvals=tickvals, ticktext=ticktext)
                elif time_format_option == 'mixed':
                    # For mixed, can't easily do tickformat, might rely on hover or just keep raw seconds
                    # or only apply custom ticktext if it's not log scale.
                    # Best to leave Plotly's default smart ticks or simplify if log scale is not on.
                    pass # Keep default for mixed, or apply custom text on hover only

                st.plotly_chart(fig_duration_hist, use_container_width=True)
            else:
                st.info("No refresh duration data within selected range for histogram.", icon="‚ÑπÔ∏è")
        else:
            st.info("No REFRESH_DURATION_SEC data available.", icon="‚ÑπÔ∏è")


    with chart_cols_state_row3[1]: # Row Change Overview (Grouped Bar Chart)
        st.markdown("<p style='font-size:16px;'><b>Row Change Overview</b></p>", unsafe_allow_html=True)
        st.write("Total inserted, deleted, and copied rows per refresh action.")
        # Ensure 'NUMCOPIEDROWS' is explicitly part of the group for completeness
        row_change_analysis_cols = ['NUMINSERTEDROWS', 'NUMDELETEDROWS', 'NUMCOPIEDROWS', 'NUMADDEDPARTITIONS', 'NUMREMOVEDPARTITIONS']
        
        # Filter for columns that actually exist in the DataFrame
        available_row_analysis_cols = [col for col in row_change_analysis_cols if col in filtered_history_df.columns]

        if available_row_analysis_cols and not filtered_history_df.empty:
            row_change_summary = filtered_history_df.groupby('REFRESH_ACTION')[available_row_analysis_cols].sum().reset_index()

            if not row_change_summary.empty:
                row_change_melted = row_change_summary.melt(
                    id_vars='REFRESH_ACTION',
                    value_vars=available_row_analysis_cols,
                    var_name='Row_Metric',
                    value_name='Count'
                )
                fig_row_change = px.bar(
                    row_change_melted,
                    x='REFRESH_ACTION',
                    y='Count',
                    color='Row_Metric',
                    title='Row Changes by Refresh Action',
                    barmode='group',
                    labels={'REFRESH_ACTION': 'Refresh Action', 'Count': 'Number of Rows'}
                )

                use_log_scale_row_change = st.checkbox("Log Scale Y-axis (Row Changes)", key="log_scale_row_change_dt_state")
                if use_log_scale_row_change:
                    fig_row_change.update_yaxes(type='log', rangemode='tozero')
                    st.info("Logarithmic scale applied to Y-axis.", icon="‚ÑπÔ∏è")

                st.plotly_chart(fig_row_change, use_container_width=True)
            else:
                st.info("No row change data after aggregation.", icon="‚ÑπÔ∏è")
        else:
            st.info("Row change statistics (NUMINSERTEDROWS, etc.) not available or no data after filters.", icon="‚ÑπÔ∏è")
    
    st.divider()

    # --- Failure & Anomaly Detection ---
    st.subheader("Failure & Anomaly Detection")

    chart_cols_state_row4 = st.columns([1, 1])

    with chart_cols_state_row4[0]: # Daily / Hourly Failure Count Trend
        st.markdown("<p style='font-size:16px;'><b>Daily Failure Count Trend</b></p>", unsafe_allow_html=True)
        st.write("Count of failed/cancelled refreshes over time.")
        failed_trend_df = filtered_history_df[
            filtered_history_df['STATE'].isin(['FAILED', 'CANCELLED', 'UPSTREAM_FAILED'])
        ].copy()
        if not failed_trend_df.empty:
            failure_counts_daily = failed_trend_df.groupby(
                pd.Grouper(key='DATA_TIMESTAMP_DT', freq='D')
            ).size().reset_index(name='Failure Count')
            failure_counts_daily.rename(columns={'DATA_TIMESTAMP_DT': 'Date'}, inplace=True)

            fig_failure_trend = px.line(
                failure_counts_daily,
                x='Date',
                y='Failure Count',
                title='Daily Failed/Cancelled Refreshes',
                labels={'Failure Count': 'Number of Failures'}
            )

            use_log_scale_failure_trend = st.checkbox("Log Scale Y-axis (Failure Trend)", key="log_scale_failure_trend_dt_state")
            if use_log_scale_failure_trend:
                fig_failure_trend.update_yaxes(type='log', rangemode='tozero')
                st.info("Logarithmic scale applied to Y-axis.", icon="‚ÑπÔ∏è")

            st.plotly_chart(fig_failure_trend, use_container_width=True)
        else:
            st.info("No failed or cancelled refreshes to trend.", icon="‚ÑπÔ∏è")

    with chart_cols_state_row4[1]: # Refresh Trigger Distribution
        st.markdown("<p style='font-size:16px;'><b>Refresh Trigger Distribution</b></p>", unsafe_allow_html=True)
        st.write("Distribution of refresh triggers (e.g., SCHEDULED, MANUAL).")
        if 'REFRESH_TRIGGER' in filtered_history_df.columns and not filtered_history_df.empty:
            trigger_counts = filtered_history_df['REFRESH_TRIGGER'].value_counts().reset_index()
            trigger_counts.columns = ['Trigger', 'Count']
            fig_trigger_dist = px.pie(
                trigger_counts,
                values='Count',
                names='Trigger',
                title='Refresh Trigger Distribution',
                hole=0.3
            )
            fig_trigger_dist.update_traces(textposition='inside', textinfo='percent+label')
            st.plotly_chart(fig_trigger_dist, use_container_width=True)
        else:
            st.info("No REFRESH_TRIGGER data available.", icon="‚ÑπÔ∏è")
    
    st.divider()

    # --- Detailed Failed Refreshes Table ---
    st.subheader("Failed & Cancelled Refreshes Details")
    st.write("Detailed list of dynamic table refreshes that did not succeed.")

    failed_refreshes_df = filtered_history_df[
        filtered_history_df['STATE'].isin(['FAILED', 'CANCELLED', 'UPSTREAM_FAILED'])
    ].copy()

    if not failed_refreshes_df.empty:
        # Checkbox for "Show latest per table"
        show_latest_failed_per_table = st.checkbox("Show only the latest failed/cancelled refresh per table", key="latest_failed_per_table")

        if show_latest_failed_per_table:
            # Sort by QUALIFIED_NAME and then by DATA_TIMESTAMP_DT (descending)
            # Then group by QUALIFIED_NAME and take the first (most recent)
            failed_refreshes_to_display = failed_refreshes_df.sort_values(
                by=['QUALIFIED_NAME', 'DATA_TIMESTAMP_DT'], ascending=[True, False]
            ).groupby('QUALIFIED_NAME').first().reset_index()
            st.info("Displaying the latest failed/cancelled refresh for each affected table.", icon="‚ÑπÔ∏è")
        else:
            failed_refreshes_to_display = failed_refreshes_df.sort_values('DATA_TIMESTAMP_DT', ascending=False)
        

        display_cols = [
            'QUALIFIED_NAME', 'STATE', 'STATE_CODE', 'STATE_MESSAGE', 'QUERY_ID',
            'DATA_TIMESTAMP_DT', 'REFRESH_START_TIME_DT', 'REFRESH_END_TIME_DT',
            'REFRESH_DURATION_SEC', 'REFRESH_ACTION', 'REFRESH_TRIGGER'
        ]
        
        if 'REFRESH_DURATION_SEC' in failed_refreshes_to_display.columns:
            failed_refreshes_to_display['REFRESH_DURATION_FMT'] = format_seconds_to_readable(
                failed_refreshes_to_display['REFRESH_DURATION_SEC'], time_format_option
            )
            display_cols = [col if col != 'REFRESH_DURATION_SEC' else 'REFRESH_DURATION_FMT' for col in display_cols]

        display_labels = {
            'QUALIFIED_NAME': 'Dynamic Table',
            'STATE': 'State',
            'STATE_CODE': 'Code',
            'STATE_MESSAGE': 'Message',
            'QUERY_ID': 'Query ID',
            'DATA_TIMESTAMP_DT': 'Data Timestamp',
            'REFRESH_START_TIME_DT': 'Refresh Start',
            'REFRESH_END_TIME_DT': 'Refresh End',
            'REFRESH_DURATION_FMT': 'Duration',
            'REFRESH_ACTION': 'Action',
            'REFRESH_TRIGGER': 'Trigger'
        }

        final_display_df = failed_refreshes_to_display[
            [col for col in display_cols if col in failed_refreshes_to_display.columns]
        ].rename(columns=display_labels)

        st.dataframe(final_display_df, use_container_width=True)
    else:
        st.info("No failed or cancelled refreshes found based on current filters. Great news!", icon="üéâ")

    st.divider()

    # --- Top N Longest Refreshes Table ---
    st.subheader("Top Longest Refreshes")
    st.write("Identifies historical refresh events with the longest durations.")

    longest_refreshes_df = filtered_history_df[
        (filtered_history_df['STATE'] == 'SUCCEEDED') &
        (filtered_history_df['REFRESH_DURATION_SEC'].notna())
    ].copy()

    if not longest_refreshes_df.empty:
        # Checkbox for "Show longest per table"
        show_longest_per_table = st.checkbox("Show only the longest refresh per table", key="longest_refresh_per_table")

        if show_longest_per_table:
            # Sort by QUALIFIED_NAME and then by REFRESH_DURATION_SEC (descending)
            # Then group by QUALIFIED_NAME and take the first (longest)
            longest_refreshes_to_display = longest_refreshes_df.sort_values(
                by=['QUALIFIED_NAME', 'REFRESH_DURATION_SEC'], ascending=[True, False]
            ).groupby('QUALIFIED_NAME').first().reset_index()
            st.info("Displaying the longest refresh for each successful table.", icon="‚ÑπÔ∏è")
        else:
            longest_refreshes_to_display = longest_refreshes_df.sort_values('REFRESH_DURATION_SEC', ascending=False).head(10)
            
        
        longest_refreshes_to_display['REFRESH_DURATION_FMT'] = format_seconds_to_readable(
            longest_refreshes_to_display['REFRESH_DURATION_SEC'], time_format_option
        )

        display_cols_longest = [
            'QUALIFIED_NAME', 'REFRESH_DURATION_FMT', 'REFRESH_ACTION', 'REFRESH_TRIGGER',
            'REFRESH_START_TIME_DT', 'REFRESH_END_TIME_DT', 'NUMINSERTEDROWS', 'NUMDELETEDROWS'
        ]

        display_labels_longest = {
            'QUALIFIED_NAME': 'Dynamic Table',
            'REFRESH_DURATION_FMT': 'Duration',
            'REFRESH_ACTION': 'Action',
            'REFRESH_TRIGGER': 'Trigger',
            'REFRESH_START_TIME_DT': 'Start Time',
            'REFRESH_END_TIME_DT': 'End Time',
            'NUMINSERTEDROWS': 'Ins. Rows',
            'NUMDELETEDROWS': 'Del. Rows'
        }

        final_display_longest_df = longest_refreshes_to_display[
            [col for col in display_cols_longest if col in longest_refreshes_to_display.columns]
        ].rename(columns=display_labels_longest)

        st.dataframe(final_display_longest_df, use_container_width=True)
    else:
        st.info("No successful refreshes found to identify longest durations.", icon="‚ÑπÔ∏è")
    
    st.divider()

    # --- Top N Frequent Failure Messages / Codes Table ---
    st.subheader("Frequent Failure Patterns")
    st.write("Identifies recurring error messages or codes from failed refreshes.")

    failure_patterns_df = filtered_history_df[
        filtered_history_df['STATE'].isin(['FAILED', 'CANCELLED', 'UPSTREAM_FAILED'])
    ].copy()

    if not failure_patterns_df.empty:
        # Group by STATE_CODE and STATE_MESSAGE, count occurrences, and get the last occurrence time
        failure_summary = failure_patterns_df.groupby(['STATE_CODE', 'STATE_MESSAGE']).agg(
            Occurrence_Count=('STATE_CODE', 'size'), # Count occurrences
            Last_Occurred_Time=('DATA_TIMESTAMP_DT', 'max') # Get latest timestamp
        ).reset_index()
        
        failure_summary = failure_summary.sort_values('Occurrence_Count', ascending=False)

        # Select columns and rename for display
        display_cols_failure = ['STATE_CODE', 'STATE_MESSAGE', 'Occurrence_Count', 'Last_Occurred_Time']
        display_labels_failure = {
            'STATE_CODE': 'Error Code',
            'STATE_MESSAGE': 'Error Message',
            'Occurrence_Count': 'Count',
            'Last_Occurred_Time': 'Last Occurred'
        }

        final_display_failure_df = failure_summary[
            [col for col in display_cols_failure if col in failure_summary.columns]
        ].rename(columns=display_labels_failure)

        st.dataframe(final_display_failure_df, use_container_width=True)
    else:
        st.info("No failed or cancelled refreshes to analyze for frequent patterns.", icon="‚ÑπÔ∏è")
