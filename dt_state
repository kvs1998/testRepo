# tabs/dt_state_tab.py
import streamlit as st
import pandas as pd
import plotly.express as px
import numpy as np

# Assuming format_seconds_to_readable is in your utils or is a standalone helper
# For this example, I'll put it here temporarily. In a real app, you might centralize helpers.
def format_seconds_to_readable(seconds_series, format_type):
    if format_type == "seconds":
        return seconds_series.round(1).astype(str) + "s"
    elif format_type == "minutes":
        return (seconds_series / 60).round(1).astype(str) + "m"
    elif format_type == "hours":
        return (seconds_series / 3600).round(1).astype(str) + "h"
    elif format_type == "days":
        return (seconds_series / 86400).round(1).astype(str) + "d"
    elif format_type == "mixed":
        def mix_format(s):
            if pd.isna(s) or s is None: return "N/A"
            s = float(s)
            if s == 0: return "0s"

            days = int(s // 86400)
            hours = int((s % 86400) // 3600)
            minutes = int((s % 3600) // 60)
            seconds = s % 60

            parts = []
            if days > 0: parts.append(f"{days}d")
            if hours > 0: parts.append(f"{hours}h")
            if minutes > 0: parts.append(f"{minutes}m")
            if seconds > 0 and (not parts or seconds >= 1):
                parts.append(f"{seconds:.1f}s")

            return " ".join(parts) if parts else "0s"
        return seconds_series.apply(mix_format)
    return seconds_series


def render_dt_state_tab(history_df: pd.DataFrame):
    st.header("Dynamic Table Refresh State & History")
    st.write("Analyze the refresh patterns, durations, and outcomes of your dynamic tables over time.")

    if history_df.empty:
        st.info("No refresh history data available. Check data source or collection.", icon="ℹ️")
        return

    # --- Data Preprocessing ---
    for col in ['REFRESH_START_TIME', 'REFRESH_END_TIME', 'DATA_TIMESTAMP']:
        if col in history_df.columns:
            history_df[f'{col}_DT'] = pd.to_datetime(history_df[col], errors='coerce')

    if 'REFRESH_START_TIME_DT' in history_df.columns and 'REFRESH_END_TIME_DT' in history_df.columns:
        history_df['REFRESH_DURATION_SEC'] = (
            history_df['REFRESH_END_TIME_DT'] - history_df['REFRESH_START_TIME_DT']
        ).dt.total_seconds().fillna(0)


    # --- Filters for DT State Tab ---
    st.markdown("---")
    st.subheader("Apply Filters for Refresh History")

    filter_cols_dt_state_row1 = st.columns([1, 1, 1, 1])

    with filter_cols_dt_state_row1[0]:
        all_databases_dt_state = ['All'] + sorted(history_df['DATABASE_NAME'].unique().tolist())
        selected_database_dt_state = st.selectbox(
            "Database:", options=all_databases_dt_state, key="db_filter_dt_state"
        )
    with filter_cols_dt_state_row1[1]:
        temp_df_for_schema_options = history_df.copy()
        if selected_database_dt_state != 'All':
            temp_df_for_schema_options = temp_df_for_schema_options[temp_df_for_schema_options['DATABASE_NAME'] == selected_database_dt_state]

        if not temp_df_for_schema_options.empty:
            schemas_in_db_dt_state = ['All'] + sorted(temp_df_for_schema_options['SCHEMA_NAME'].unique().tolist())
        else:
            schemas_in_db_dt_state = ['All']
            st.info("No schemas found for selected Database.", icon="ℹ️")

        selected_schema_dt_state = st.selectbox(
            "Schema:", options=schemas_in_db_dt_state, key="schema_filter_dt_state"
        )

    df_for_table_options_state = history_df.copy()
    if selected_database_dt_state != 'All':
        df_for_table_options_state = df_for_table_options_state[df_for_table_options_state['DATABASE_NAME'] == selected_database_dt_state]
    if selected_schema_dt_state != 'All':
        df_for_table_options_state = df_for_table_options_state[df_for_table_options_state['SCHEMA_NAME'] == selected_schema_dt_state]


    with filter_cols_dt_state_row1[2]:
        if not df_for_table_options_state.empty:
            all_tables_dt_state_options = ['All'] + sorted(df_for_table_options_state['TABLE_NAME'].unique().tolist())
            default_tables_dt_state_selected = ['All']
        else:
            all_tables_dt_state_options = ['All']
            default_tables_dt_state_selected = ['All']
            st.info("No tables found for selected DB/Schema.", icon="ℹ️")
        selected_table_dt_state = st.multiselect(
            "Table(s):", options=all_tables_dt_state_options, default=default_tables_dt_state_selected, key="table_filter_dt_state"
        )

    with filter_cols_dt_state_row1[3]:
        all_states = ['All'] + sorted(history_df['STATE'].unique().tolist())
        selected_state = st.multiselect(
            "Refresh State(s):", options=all_states, default=['All'], key="refresh_state_filter_dt_state"
        )

    filter_cols_dt_state_row2 = st.columns([1, 1])

    with filter_cols_dt_state_row2[0]:
        time_range = st.date_input(
            "Select Date Range for History:",
            value=(history_df['DATA_TIMESTAMP_DT'].min(), history_df['DATA_TIMESTAMP_DT'].max()),
            min_value=history_df['DATA_TIMESTAMP_DT'].min(),
            max_value=history_df['DATA_TIMESTAMP_DT'].max(),
            key="date_range_dt_state"
        )
        # Ensure time_range is a tuple of 2 dates
        if isinstance(time_range, tuple) and len(time_range) == 2:
            start_date, end_date = time_range
        elif isinstance(time_range, list) and len(time_range) == 2: # handle list output if any
            start_date, end_date = time_range
        elif isinstance(time_range, (pd.Timestamp, datetime.date)): # handle single date selection
            start_date, end_date = time_range, time_range
        else: # Default to full range if nothing selected or error
            start_date, end_date = history_df['DATA_TIMESTAMP_DT'].min(), history_df['DATA_TIMESTAMP_DT'].max()
        # Ensure start_date and end_date are datetime.date objects for comparison
        start_date = pd.to_datetime(start_date).date()
        end_date = pd.to_datetime(end_date).date()


    with filter_cols_dt_state_row2[1]:
        time_format_option = st.radio(
            "Display Durations In:",
            options=["mixed", "seconds", "minutes", "hours", "days"],
            index=0,
            horizontal=True,
            key="duration_format_dt_state"
        )
    st.markdown("---")

    # --- Apply Filters to Main DataFrame ---
    filtered_history_df = history_df.copy()

    if selected_database_dt_state != 'All':
        filtered_history_df = filtered_history_df[filtered_history_df['DATABASE_NAME'] == selected_database_dt_state].copy()
    if selected_schema_dt_state != 'All':
        filtered_history_df = filtered_history_df[filtered_history_df['SCHEMA_NAME'] == selected_schema_dt_state].copy()
    if selected_table_dt_state and 'All' not in selected_table_dt_state:
        filtered_history_df = filtered_history_df[filtered_history_df['TABLE_NAME'].isin(selected_table_dt_state)].copy()
    elif not selected_table_dt_state:
        st.warning("No table(s) selected. Display will be empty.", icon="⚠️")
        filtered_history_df = pd.DataFrame()

    if selected_state and 'All' not in selected_state:
        filtered_history_df = filtered_history_df[filtered_history_df['STATE'].isin(selected_state)].copy()
    elif not selected_state:
        st.warning("No refresh state(s) selected. Display will be empty.", icon="⚠️")
        filtered_history_df = pd.DataFrame()

    if start_date and end_date:
        filtered_history_df = filtered_history_df[
            (filtered_history_df['DATA_TIMESTAMP_DT'].dt.date >= start_date) &
            (filtered_history_df['DATA_TIMESTAMP_DT'].dt.date <= end_date)
        ].copy()

    if filtered_history_df.empty:
        st.info("No data available based on current filter selections. Please adjust your filters.", icon="ℹ️")
        return

    # --- Refresh Trends & Distributions ---
    st.subheader("Refresh Trends & Status Over Time")
    chart_cols_state_row1 = st.columns([1, 1])

    with chart_cols_state_row1[0]:
        st.markdown("<p style='font-size:16px;'><b>Daily Refresh Status Trend</b></p>", unsafe_allow_html=True)
        st.write("Count of refreshes by state over the selected time range.")
        if not filtered_history_df.empty:
            # Aggregate by date and state
            status_trend_df = filtered_history_df.groupby([
                pd.Grouper(key='DATA_TIMESTAMP_DT', freq='D'), 'STATE'
            ]).size().unstack(fill_value=0).reset_index()
            status_trend_df.rename(columns={'DATA_TIMESTAMP_DT': 'Date'}, inplace=True)

            state_order = ['SUCCEEDED', 'FAILED', 'UPSTREAM_FAILED', 'CANCELLED', 'EXECUTING', 'SCHEDULED']
            present_states = [s for s in state_order if s in status_trend_df.columns]

            status_trend_melted = status_trend_df.melt(
                id_vars=['Date'],
                value_vars=present_states,
                var_name='State',
                value_name='Count'
            )

            fig_status_trend = px.bar(
                status_trend_melted,
                x='Date',
                y='Count',
                color='State',
                title='Daily Refresh Status Trend',
                color_discrete_map={
                    'SUCCEEDED': 'green', 'FAILED': 'red', 'UPSTREAM_FAILED': 'darkred',
                    'CANCELLED': 'orange', 'EXECUTING': 'blue', 'SCHEDULED': 'grey'
                }
            )
            fig_status_trend.update_layout(barmode='stack')

            use_log_scale_refresh_status = st.checkbox("Log Scale Y-axis (Refresh Status Trend)", key="log_scale_refresh_status_dt_state")
            if use_log_scale_refresh_status:
                fig_status_trend.update_yaxes(type='log')
                st.info("Logarithmic scale applied to Y-axis.", icon="ℹ️")

            st.plotly_chart(fig_status_trend, use_container_width=True)
        else:
            st.info("No data for Refresh Status Trend.", icon="ℹ️")

    with chart_cols_state_row1[1]:
        st.markdown("<p style='font-size:16px;'><b>Refresh Action Distribution</b></p>", unsafe_allow_html=True)
        st.write("Distribution of refresh actions (e.g., INCREMENTAL, FULL, NO_DATA).")
        if 'REFRESH_ACTION' in filtered_history_df.columns and not filtered_history_df.empty:
            action_counts = filtered_history_df['REFRESH_ACTION'].value_counts().reset_index()
            action_counts.columns = ['Action', 'Count']
            fig_action_dist = px.pie(
                action_counts,
                values='Count',
                names='Action',
                title='Refresh Action Distribution',
                hole=0.3
            )
            fig_action_dist.update_traces(textposition='inside', textinfo='percent+label')
            st.plotly_chart(fig_action_dist, use_container_width=True)
        else:
            st.info("No REFRESH_ACTION data available.", icon="ℹ️")

    st.divider()

    # --- Performance & Efficiency Analysis ---
    st.subheader("Performance & Efficiency Analysis")
    chart_cols_state_row2 = st.columns([1, 1])

    with chart_cols_state_row2[0]: # Refresh Duration Trend (Line Chart with Averages)
        st.markdown("<p style='font-size:16px;'><b>Refresh Duration Trend</b></p>", unsafe_allow_html=True)
        st.write("Average and Median refresh duration over time.")
        if 'REFRESH_DURATION_SEC' in filtered_history_df.columns and not filtered_history_df['REFRESH_DURATION_SEC'].empty:
            # Aggregate daily average and median duration
            duration_trend_df = filtered_history_df.groupby(
                pd.Grouper(key='DATA_TIMESTAMP_DT', freq='D')
            )['REFRESH_DURATION_SEC'].agg(['mean', 'median', lambda x: x.quantile(0.95)]).reset_index()
            duration_trend_df.columns = ['Date', 'Mean Duration', 'Median Duration', 'P95 Duration']

            fig_duration_trend = px.line(
                duration_trend_df,
                x='Date',
                y=['Mean Duration', 'Median Duration', 'P95 Duration'], # Plot multiple lines
                title='Daily Refresh Duration Trend (Mean, Median, P95)',
                labels={'value': 'Duration (seconds)'}
            )

            use_log_scale_duration_trend = st.checkbox("Log Scale Y-axis (Duration Trend)", key="log_scale_duration_trend_dt_state")
            if use_log_scale_duration_trend:
                fig_duration_trend.update_yaxes(type='log')
                st.info("Logarithmic scale applied to Y-axis.", icon="ℹ️")

            st.plotly_chart(fig_duration_trend, use_container_width=True)
        else:
            st.info("No REFRESH_DURATION_SEC data available for trend analysis.", icon="ℹ️")


    with chart_cols_state_row2[1]: # Refresh Duration vs. Rows Processed (Scatter Plot)
        st.markdown("<p style='font-size:16px;'><b>Refresh Duration vs. Rows Processed</b></p>", unsafe_allow_html=True)
        st.write("Examine if duration correlates with data volume.")
        row_change_cols = ['NUMINSERTEDROWS', 'NUMDELETEDROWS', 'NUMCOPIEDROWS']
        if 'REFRESH_DURATION_SEC' in filtered_history_df.columns and \
           all(col in filtered_history_df.columns for col in row_change_cols):
            
            scatter_df = filtered_history_df.dropna(subset=['REFRESH_DURATION_SEC'] + row_change_cols).copy()
            scatter_df['TOTAL_ROWS_CHANGED'] = scatter_df[row_change_cols].sum(axis=1)

            # Ensure some data exists after filtering
            if not scatter_df.empty and scatter_df['TOTAL_ROWS_CHANGED'].sum() > 0:
                fig_duration_vs_rows = px.scatter(
                    scatter_df,
                    x='TOTAL_ROWS_CHANGED',
                    y='REFRESH_DURATION_SEC',
                    color='STATE', # Color by refresh state
                    hover_name='QUALIFIED_NAME', # Show table name on hover
                    hover_data=['REFRESH_ACTION', 'REFRESH_START_TIME_DT', 'REFRESH_END_TIME_DT', 'STATE_MESSAGE'],
                    title='Refresh Duration vs. Total Rows Changed',
                    labels={'REFRESH_DURATION_SEC': 'Duration (seconds)', 'TOTAL_ROWS_CHANGED': 'Total Rows Changed'}
                )

                log_x_scatter = st.checkbox("Log Scale X-axis (Rows Processed)", key="log_x_scatter_dt_state")
                log_y_scatter = st.checkbox("Log Scale Y-axis (Duration Scatter)", key="log_y_scatter_dt_state")

                if log_x_scatter:
                    fig_duration_vs_rows.update_xaxes(type='log')
                if log_y_scatter:
                    fig_duration_vs_rows.update_yaxes(type='log')
                
                if log_x_scatter or log_y_scatter:
                    st.info("Logarithmic scale applied to selected axis/axes.", icon="ℹ️")

                st.plotly_chart(fig_duration_vs_rows, use_container_width=True)
            else:
                st.info("No valid data for Refresh Duration vs. Rows Processed scatter plot after filters.", icon="ℹ️")
        else:
            st.info("Necessary columns (REFRESH_DURATION_SEC, NUMINSERTEDROWS, etc.) not available for this analysis.", icon="ℹ️")
    
    st.divider()

    chart_cols_state_row3 = st.columns([1, 1])

    with chart_cols_state_row3[0]: # Refresh Duration Distribution (Histogram)
        st.markdown("<p style='font-size:16px;'><b>Refresh Duration Distribution</b></p>", unsafe_allow_html=True)
        st.write("Distribution of refresh durations.")
        if 'REFRESH_DURATION_SEC' in filtered_history_df.columns and not filtered_history_df['REFRESH_DURATION_SEC'].empty:
            duration_data = filtered_history_df['REFRESH_DURATION_SEC'].dropna()

            max_duration = duration_data.max() if not duration_data.empty else 60

            duration_max_limit_sec = st.slider(
                f"Max Duration for Histogram (seconds):",
                min_value=0.0,
                max_value=float(max_duration),
                value=min(float(max_duration), 3600.0), # Default to 1 hour or max if less
                step=10.0,
                format="%.0f s",
                key="duration_hist_limit"
            )

            duration_data_for_hist = duration_data[duration_data <= duration_max_limit_sec]

            if not duration_data_for_hist.empty:
                fig_duration_hist = px.histogram(
                    duration_data_for_hist,
                    x="REFRESH_DURATION_SEC",
                    nbins=20,
                    title='Refresh Duration Distribution',
                    labels={'REFRESH_DURATION_SEC': 'Duration (seconds)'}
                )
                fig_duration_hist.update_layout(bargap=0.1)

                use_log_scale_duration_hist = st.checkbox("Log Scale X-axis (Duration Histogram)", key="log_scale_duration_hist_dt_state")
                if use_log_scale_duration_hist:
                    fig_duration_hist.update_xaxes(type='log')
                    st.info("Logarithmic scale applied to X-axis.", icon="ℹ️")

                st.plotly_chart(fig_duration_hist, use_container_width=True)
            else:
                st.info("No refresh duration data within selected range for histogram.", icon="ℹ️")
        else:
            st.info("No REFRESH_DURATION_SEC data available.", icon="ℹ️")


    with chart_cols_state_row3[1]: # Row Change Overview (Grouped Bar Chart)
        st.markdown("<p style='font-size:16px;'><b>Row Change Overview</b></p>", unsafe_allow_html=True)
        st.write("Total inserted, deleted, and copied rows per refresh action.")
        row_change_cols = ['NUMINSERTEDROWS', 'NUMDELETEDROWS', 'NUMCOPIEDROWS']
        if all(col in filtered_history_df.columns for col in row_change_cols):
            row_change_summary = filtered_history_df.groupby('REFRESH_ACTION')[row_change_cols].sum().reset_index()
            # Ensure numbers are actual numbers, fillna with 0 for summation
            for col in row_change_cols:
                row_change_summary[col] = pd.to_numeric(row_change_summary[col], errors='coerce').fillna(0)

            if not row_change_summary.empty:
                row_change_melted = row_change_summary.melt(
                    id_vars='REFRESH_ACTION',
                    value_vars=row_change_cols,
                    var_name='Row_Metric',
                    value_name='Count'
                )
                fig_row_change = px.bar(
                    row_change_melted,
                    x='REFRESH_ACTION',
                    y='Count',
                    color='Row_Metric',
                    title='Row Changes by Refresh Action',
                    barmode='group',
                    labels={'REFRESH_ACTION': 'Refresh Action', 'Count': 'Number of Rows'}
                )

                use_log_scale_row_change = st.checkbox("Log Scale Y-axis (Row Changes)", key="log_scale_row_change_dt_state")
                if use_log_scale_row_change:
                    fig_row_change.update_yaxes(type='log')
                    st.info("Logarithmic scale applied to Y-axis.", icon="ℹ️")

                st.plotly_chart(fig_row_change, use_container_width=True)
            else:
                st.info("No row change data after aggregation.", icon="ℹ️")
        else:
            st.info("Row change statistics (NUMINSERTEDROWS, etc.) not available or not properly flattened.", icon="ℹ️")
    st.divider()

    # --- Failure & Anomaly Detection ---
    st.subheader("Failure & Anomaly Detection")

    chart_cols_state_row4 = st.columns([1, 1])

    with chart_cols_state_row4[0]: # Daily / Hourly Failure Count Trend
        st.markdown("<p style='font-size:16px;'><b>Daily Failure Count Trend</b></p>", unsafe_allow_html=True)
        st.write("Count of failed/cancelled refreshes over time.")
        failed_trend_df = filtered_history_df[
            filtered_history_df['STATE'].isin(['FAILED', 'CANCELLED', 'UPSTREAM_FAILED'])
        ].copy()
        if not failed_trend_df.empty:
            failure_counts_daily = failed_trend_df.groupby(
                pd.Grouper(key='DATA_TIMESTAMP_DT', freq='D')
            ).size().reset_index(name='Failure Count')
            failure_counts_daily.rename(columns={'DATA_TIMESTAMP_DT': 'Date'}, inplace=True)

            fig_failure_trend = px.line(
                failure_counts_daily,
                x='Date',
                y='Failure Count',
                title='Daily Failed/Cancelled Refreshes',
                labels={'Failure Count': 'Number of Failures'}
            )

            use_log_scale_failure_trend = st.checkbox("Log Scale Y-axis (Failure Trend)", key="log_scale_failure_trend_dt_state")
            if use_log_scale_failure_trend:
                fig_failure_trend.update_yaxes(type='log')
                st.info("Logarithmic scale applied to Y-axis.", icon="ℹ️")

            st.plotly_chart(fig_failure_trend, use_container_width=True)
        else:
            st.info("No failed or cancelled refreshes to trend.", icon="ℹ️")

    with chart_cols_state_row4[1]: # Refresh Trigger Distribution
        st.markdown("<p style='font-size:16px;'><b>Refresh Trigger Distribution</b></p>", unsafe_allow_html=True)
        st.write("Distribution of refresh triggers (e.g., SCHEDULED, MANUAL).")
        if 'REFRESH_TRIGGER' in filtered_history_df.columns and not filtered_history_df.empty:
            trigger_counts = filtered_history_df['REFRESH_TRIGGER'].value_counts().reset_index()
            trigger_counts.columns = ['Trigger', 'Count']
            fig_trigger_dist = px.pie(
                trigger_counts,
                values='Count',
                names='Trigger',
                title='Refresh Trigger Distribution',
                hole=0.3
            )
            fig_trigger_dist.update_traces(textposition='inside', textinfo='percent+label')
            st.plotly_chart(fig_trigger_dist, use_container_width=True)
        else:
            st.info("No REFRESH_TRIGGER data available.", icon="ℹ️")
    
    st.divider()

    # --- Detailed Failed Refreshes Table ---
    st.subheader("Failed & Cancelled Refreshes Details")
    st.write("Detailed list of dynamic table refreshes that did not succeed.")

    failed_refreshes_df = filtered_history_df[
        filtered_history_df['STATE'].isin(['FAILED', 'CANCELLED', 'UPSTREAM_FAILED'])
    ].copy()

    if not failed_refreshes_df.empty:
        # Sort by REFRESH_END_TIME (descending) or DATA_TIMESTAMP_DT
        failed_refreshes_df = failed_refreshes_df.sort_values(
            'DATA_TIMESTAMP_DT', ascending=False
        )

        # Select columns to display in the table
        display_cols = [
            'QUALIFIED_NAME', 'STATE', 'STATE_CODE', 'STATE_MESSAGE', 'QUERY_ID',
            'DATA_TIMESTAMP_DT', 'REFRESH_START_TIME_DT', 'REFRESH_END_TIME_DT',
            'REFRESH_DURATION_SEC', 'REFRESH_ACTION', 'REFRESH_TRIGGER'
        ]
        
        # Apply formatting for duration
        if 'REFRESH_DURATION_SEC' in failed_refreshes_df.columns:
            failed_refreshes_df['REFRESH_DURATION_FMT'] = format_seconds_to_readable(
                failed_refreshes_df['REFRESH_DURATION_SEC'], time_format_option
            )
            # Replace the raw duration column with the formatted one for display
            display_cols = [col if col != 'REFRESH_DURATION_SEC' else 'REFRESH_DURATION_FMT' for col in display_cols]


        # Rename columns for display
        display_labels = {
            'QUALIFIED_NAME': 'Dynamic Table',
            'STATE': 'State',
            'STATE_CODE': 'Code',
            'STATE_MESSAGE': 'Message',
            'QUERY_ID': 'Query ID',
            'DATA_TIMESTAMP_DT': 'Data Timestamp',
            'REFRESH_START_TIME_DT': 'Refresh Start',
            'REFRESH_END_TIME_DT': 'Refresh End',
            'REFRESH_DURATION_FMT': 'Duration',
            'REFRESH_ACTION': 'Action',
            'REFRESH_TRIGGER': 'Trigger'
        }

        # Filter to only existing columns and rename
        final_display_df = failed_refreshes_df[
            [col for col in display_cols if col in failed_refreshes_df.columns]
        ].rename(columns=display_labels)

        st.dataframe(final_display_df, use_container_width=True)
    else:
        st.info("No failed or cancelled refreshes found based on current filters. Great news!", icon="🎉")

    st.divider()

    # --- Top N Longest Refreshes Table ---
    st.subheader("Top Longest Refreshes")
    st.write("Identifies historical refresh events with the longest durations.")

    # Filter for successful refreshes that have a duration
    longest_refreshes_df = filtered_history_df[
        (filtered_history_df['STATE'] == 'SUCCEEDED') &
        (filtered_history_df['REFRESH_DURATION_SEC'].notna())
    ].copy()

    if not longest_refreshes_df.empty:
        longest_refreshes_df = longest_refreshes_df.sort_values(
            'REFRESH_DURATION_SEC', ascending=False
        ).head(10) # Show top 10 by default

        # Apply formatting for duration
        longest_refreshes_df['REFRESH_DURATION_FMT'] = format_seconds_to_readable(
            longest_refreshes_df['REFRESH_DURATION_SEC'], time_format_option
        )

        display_cols_longest = [
            'QUALIFIED_NAME', 'REFRESH_DURATION_FMT', 'REFRESH_ACTION', 'REFRESH_TRIGGER',
            'REFRESH_START_TIME_DT', 'REFRESH_END_TIME_DT', 'NUMINSERTEDROWS', 'NUMDELETEDROWS'
        ]

        display_labels_longest = {
            'QUALIFIED_NAME': 'Dynamic Table',
            'REFRESH_DURATION_FMT': 'Duration',
            'REFRESH_ACTION': 'Action',
            'REFRESH_TRIGGER': 'Trigger',
            'REFRESH_START_TIME_DT': 'Start Time',
            'REFRESH_END_TIME_DT': 'End Time',
            'NUMINSERTEDROWS': 'Ins. Rows',
            'NUMDELETEDROWS': 'Del. Rows'
        }

        final_display_longest_df = longest_refreshes_df[
            [col for col in display_cols_longest if col in longest_refreshes_df.columns]
        ].rename(columns=display_labels_longest)

        st.dataframe(final_display_longest_df, use_container_width=True)
    else:
        st.info("No successful refreshes found to identify longest durations.", icon="ℹ️")
    
    st.divider()

    # --- Top N Frequent Failure Messages / Codes Table ---
    st.subheader("Frequent Failure Patterns")
    st.write("Identifies recurring error messages or codes from failed refreshes.")

    failure_patterns_df = filtered_history_df[
        filtered_history_df['STATE'].isin(['FAILED', 'CANCELLED', 'UPSTREAM_FAILED'])
    ].copy()

    if not failure_patterns_df.empty:
        # Group by STATE_CODE and STATE_MESSAGE and count occurrences
        failure_summary = failure_patterns_df.groupby(['STATE_CODE', 'STATE_MESSAGE']).size().reset_index(name='Occurrence_Count')
        failure_summary = failure_summary.sort_values('Occurrence_Count', ascending=False)

        # Select columns and rename for display
        display_cols_failure = ['STATE_CODE', 'STATE_MESSAGE', 'Occurrence_Count']
        display_labels_failure = {
            'STATE_CODE': 'Error Code',
            'STATE_MESSAGE': 'Error Message',
            'Occurrence_Count': 'Count'
        }

        final_display_failure_df = failure_summary[
            [col for col in display_cols_failure if col in failure_summary.columns]
        ].rename(columns=display_labels_failure)

        st.dataframe(final_display_failure_df, use_container_width=True)
    else:
        st.info("No failed or cancelled refreshes to analyze for frequent patterns.", icon="ℹ️")
