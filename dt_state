# tabs/dt_state_tab.py
import streamlit as st
import pandas as pd
import plotly.express as px
import numpy as np
from datetime import datetime, time, date # Import for more precise date handling


# Assuming format_seconds_to_readable is in your utils or is a standalone helper
def format_seconds_to_readable(seconds_series, format_type):
    if format_type == "seconds":
        return seconds_series.round(1).astype(str) + "s"
    elif format_type == "minutes":
        return (seconds_series / 60).round(1).astype(str) + "m"
    elif format_type == "hours":
        return (seconds_series / 3600).round(1).astype(str) + "h"
    elif format_type == "days":
        return (seconds_series / 86400).round(1).astype(str) + "d"
    elif format_type == "mixed":
        def mix_format(s):
            if pd.isna(s) or s is None: return "N/A"
            s = float(s)
            if s == 0: return "0s"

            days = int(s // 86400)
            hours = int((s % 86400) // 3600)
            minutes = int((s % 3600) // 60)
            seconds = s % 60

            parts = []
            if days > 0: parts.append(f"{days}d")
            if hours > 0: parts.append(f"{hours}h")
            if minutes > 0: parts.append(f"{minutes}m")
            if seconds > 0 and (not parts or seconds >= 1):
                parts.append(f"{seconds:.1f}s")

            return " ".join(parts) if parts else "0s"
        return seconds_series.apply(mix_format)
    return seconds_series


def render_dt_state_tab(history_df: pd.DataFrame):
    st.header("Dynamic Table Refresh State & History")
    st.write("Analyze the refresh patterns, durations, and outcomes of your dynamic tables over time.")

    if history_df.empty:
        st.info("No refresh history data available. Check data source or collection.", icon="ℹ️")
        return

    # --- Data Preprocessing ---
    for col in ['REFRESH_START_TIME', 'REFRESH_END_TIME', 'DATA_TIMESTAMP']:
        if col in history_df.columns:
            history_df[f'{col}_DT'] = pd.to_datetime(history_df[col], errors='coerce')

    if 'REFRESH_START_TIME_DT' in history_df.columns and 'REFRESH_END_TIME_DT' in history_df.columns:
        history_df['REFRESH_DURATION_SEC'] = (
            history_df['REFRESH_END_TIME_DT'] - history_df['REFRESH_START_TIME_DT']
        ).dt.total_seconds().fillna(0)

    # Ensure numerical row change columns are numeric
    row_change_raw_cols = ['NUMINSERTEDROWS', 'NUMDELETEDROWS', 'NUMCOPIEDROWS', 'NUMADDEDPARTITIONS', 'NUMREMOVEDPARTITIONS']
    for col in row_change_raw_cols:
        if col in history_df.columns:
            history_df[col] = pd.to_numeric(history_df[col], errors='coerce').fillna(0)


    # --- Filters for DT State Tab ---
    st.markdown("---")
    st.subheader("Apply Filters for Refresh History")

    filter_cols_dt_state_row1 = st.columns([1, 1, 1, 1])

    with filter_cols_dt_state_row1[0]:
        all_databases_dt_state = ['All'] + sorted(history_df['DATABASE_NAME'].unique().tolist())
        selected_database_dt_state = st.selectbox(
            "Database:", options=all_databases_dt_state, key="db_filter_dt_state"
        )
    with filter_cols_dt_state_row1[1]:
        temp_df_for_schema_options = history_df.copy()
        if selected_database_dt_state != 'All':
            temp_df_for_schema_options = temp_df_for_schema_options[temp_df_for_schema_options['DATABASE_NAME'] == selected_database_dt_state]

        if not temp_df_for_schema_options.empty:
            schemas_in_db_dt_state = ['All'] + sorted(temp_df_for_schema_options['SCHEMA_NAME'].unique().tolist())
        else:
            schemas_in_db_dt_state = ['All']
            st.info("No schemas found for selected Database.", icon="ℹ️")

        selected_schema_dt_state = st.selectbox(
            "Schema:", options=schemas_in_db_dt_state, key="schema_filter_dt_state"
        )

    df_for_table_options_state = history_df.copy()
    if selected_database_dt_state != 'All':
        df_for_table_options_state = df_for_table_options_state[df_for_table_options_state['DATABASE_NAME'] == selected_database_dt_state]
    if selected_schema_dt_state != 'All':
        df_for_table_options_state = df_for_table_options_state[df_for_table_options_state['SCHEMA_NAME'] == selected_schema_dt_state]


    with filter_cols_dt_state_row1[2]:
        if not df_for_table_options_state.empty:
            all_tables_dt_state_options = ['All'] + sorted(df_for_table_options_state['TABLE_NAME'].unique().tolist())
            default_tables_dt_state_selected = ['All']
        else:
            all_tables_dt_state_options = ['All']
            default_tables_dt_state_selected = ['All']
            st.info("No tables found for selected DB/Schema.", icon="ℹ️")
        selected_table_dt_state = st.multiselect(
            "Table(s):", options=all_tables_dt_state_options, default=default_tables_dt_state_selected, key="table_filter_dt_state"
        )

    with filter_cols_dt_state_row1[3]:
        all_states = ['All'] + sorted(history_df['STATE'].unique().tolist())
        selected_state = st.multiselect(
            "Refresh State(s):", options=all_states, default=['All'], key="refresh_state_filter_dt_state"
        )

    filter_cols_dt_state_row2 = st.columns([1, 1])

    with filter_cols_dt_state_row2[0]:
        # Robust date range handling with min_value/max_value derived from data
        min_data_date = history_df['DATA_TIMESTAMP_DT'].min().date()
        max_data_date = history_df['DATA_TIMESTAMP_DT'].max().date()
        
        # Default value should be a tuple (start_date, end_date)
        # If no data or single day, ensure it doesn't break
        default_start_date = min_data_date
        default_end_date = max_data_date

        # This widget can return a tuple (range) or a single date object
        selected_date_input = st.date_input(
            "Select Date Range for History:",
            value=(default_start_date, default_end_date),
            min_value=min_data_date,
            max_value=max_data_date,
            key="date_range_dt_state"
        )
        
        # Parse the output from st.date_input to determine start_date_filter and end_date_filter
        if isinstance(selected_date_input, tuple) and len(selected_date_input) == 2:
            start_date_filter = pd.to_datetime(selected_date_input[0]).normalize() # Start of first selected day
            end_date_filter = pd.to_datetime(selected_date_input[1]).normalize() + pd.Timedelta(days=1) - pd.Timedelta(microseconds=1) # End of last selected day
        elif isinstance(selected_date_input, (datetime.date, datetime)): # Single date selected
            start_date_filter = pd.to_datetime(selected_date_input).normalize()
            end_date_filter = pd.to_datetime(selected_date_input).normalize() + pd.Timedelta(days=1) - pd.Timedelta(microseconds=1)
        else: # Fallback if input is weird, e.g., only one date selected by user then deleted one
            start_date_filter = history_df['DATA_TIMESTAMP_DT'].min()
            end_date_filter = history_df['DATA_TIMESTAMP_DT'].max()

    with filter_cols_dt_state_row2[1]:
        time_format_option = st.radio(
            "Display Durations In:",
            options=["mixed", "seconds", "minutes", "hours", "days"],
            index=0,
            horizontal=True,
            key="duration_format_dt_state"
        )
    st.markdown("---")

    # --- Apply Filters to Main DataFrame ---
    filtered_history_df = history_df.copy()

    if selected_database_dt_state != 'All':
        filtered_history_df = filtered_history_df[filtered_history_df['DATABASE_NAME'] == selected_database_dt_state].copy()
    if selected_schema_dt_state != 'All':
        filtered_history_df = filtered_history_df[filtered_history_df['SCHEMA_NAME'] == selected_schema_dt_state].copy()
    if selected_table_dt_state and 'All' not in selected_table_dt_state:
        filtered_history_df = filtered_history_df[filtered_history_df['TABLE_NAME'].isin(selected_table_dt_state)].copy()
    elif not selected_table_dt_state:
        st.warning("No table(s) selected. Display will be empty.", icon="⚠️")
        filtered_history_df = pd.DataFrame()

    if selected_state and 'All' not in selected_state:
        filtered_history_df = filtered_history_df[filtered_history_df['STATE'].isin(selected_state)].copy()
    elif not selected_state:
        st.warning("No refresh state(s) selected. Display will be empty.", icon="⚠️")
        filtered_history_df = pd.DataFrame()

    # Apply date range filter using Timestamps directly
    # Ensure all data in DATA_TIMESTAMP_DT column has timezone removed or set to UTC for consistent comparison
    if 'DATA_TIMESTAMP_DT' in filtered_history_df.columns:
        filtered_history_df = filtered_history_df[
            (filtered_history_df['DATA_TIMESTAMP_DT'] >= start_date_filter) &
            (filtered_history_df['DATA_TIMESTAMP_DT'] <= end_date_filter)
        ].copy()

    if filtered_history_df.empty:
        st.info("No data available based on current filter selections. Please adjust your filters.", icon="ℹ️")
        return

    # --- Refresh Trends & Distributions ---
    st.subheader("Refresh Trends & Status Over Time")
    chart_cols_state_row1 = st.columns([1, 1])

    with chart_cols_state_row1[0]:
        st.markdown("<p style='font-size:16px;'><b>Daily Refresh Status Trend</b></p>", unsafe_allow_html=True)
        st.write("Count of refreshes by state over the selected time range.")
        if not filtered_history_df.empty:
            status_trend_df = filtered_history_df.groupby([
                pd.Grouper(key='DATA_TIMESTAMP_DT', freq='D'), 'STATE'
            ]).size().unstack(fill_value=0).reset_index()
            status_trend_df.rename(columns={'DATA_TIMESTAMP_DT': 'Date'}, inplace=True)

            state_order = ['SUCCEEDED', 'FAILED', 'UPSTREAM_FAILED', 'CANCELLED', 'EXECUTING', 'SCHEDULED']
            present_states = [s for s in state_order if s in status_trend_df.columns]

            status_trend_melted = status_trend_df.melt(
                id_vars=['Date'],
                value_vars=present_states,
                var_name='State',
                value_name='Count'
            )

            fig_status_trend = px.bar(
                status_trend_melted,
                x='Date',
                y='Count',
                color='State',
                title='Daily Refresh Status Trend',
                color_discrete_map={
                    'SUCCEEDED': 'green', 'FAILED': 'red', 'UPSTREAM_FAILED': 'darkred',
                    'CANCELLED': 'orange', 'EXECUTING': 'blue', 'SCHEDULED': 'grey'
                }
            )
            fig_status_trend.update_layout(barmode='stack')

            use_log_scale_refresh_status = st.checkbox("Log Scale Y-axis (Refresh Status Trend)", key="log_scale_refresh_status_dt_state")
            if use_log_scale_refresh_status:
                # Add a small offset to zero counts for log scale
                fig_status_trend.update_yaxes(type='log', title='Count (Log Scale)', rangemode='tozero')
                # For stacked bar charts, Plotly handles zeroes reasonably well with rangemode='tozero'.
                st.info("Logarithmic scale applied to Y-axis. Zero counts are handled gracefully.", icon="ℹ️")

            st.plotly_chart(fig_status_trend, use_container_width=True)
        else:
            st.info("No data for Refresh Status Trend.", icon="ℹ️")

    with chart_cols_state_row1[1]:
        st.markdown("<p style='font-size:16px;'><b>Refresh Action Distribution</b></p>", unsafe_allow_html=True)
        st.write("Distribution of refresh actions (e.g., INCREMENTAL, FULL, NO_DATA).")
        if 'REFRESH_ACTION' in filtered_history_df.columns and not filtered_history_df.empty:
            action_counts = filtered_history_df['REFRESH_ACTION'].value_counts().reset_index()
            action_counts.columns = ['Action', 'Count']
            fig_action_dist = px.pie(
                action_counts,
                values='Count',
                names='Action',
                title='Refresh Action Distribution',
                hole=0.3
            )
            fig_action_dist.update_traces(textposition='inside', textinfo='percent+label')
            st.plotly_chart(fig_action_dist, use_container_width=True)
        else:
            st.info("No REFRESH_ACTION data available.", icon="ℹ️")

    st.divider()

    # --- Row Changes Over Time ---
    st.subheader("Row Changes Over Time")
    st.write("Trend of inserted, deleted, copied, added partitions, and removed partitions over time.")

    row_change_trend_cols_raw = ['NUMINSERTEDROWS', 'NUMDELETEDROWS', 'NUMCOPIEDROWS', 'NUMADDEDPARTITIONS', 'NUMREMOVEDPARTITIONS']
    row_change_trend_cols_display_map = {
        'NUMINSERTEDROWS': 'Inserted Rows',
        'NUMDELETEDROWS': 'Deleted Rows',
        'NUMCOPIEDROWS': 'Copied Rows',
        'NUMADDEDPARTITIONS': 'Added Partitions',
        'NUMREMOVEDPARTITIONS': 'Removed Partitions'
    }

    # Filter for columns that actually exist in the DataFrame
    available_row_trend_cols = [col for col in row_change_trend_cols_raw if col in filtered_history_df.columns]

    if available_row_trend_cols and not filtered_history_df.empty:
        # Aggregate daily sum for each row change metric
        row_trend_df = filtered_history_df.groupby(pd.Grouper(key='DATA_TIMESTAMP_DT', freq='D'))[available_row_trend_cols].sum().reset_index()
        row_trend_df.rename(columns={'DATA_TIMESTAMP_DT': 'Date'}, inplace=True)

        # Melt for stacked area chart
        row_trend_melted = row_trend_df.melt(
            id_vars=['Date'],
            value_vars=available_row_trend_cols,
            var_name='Row_Metric_Raw',
            value_name='Count'
        )
        # Map raw column names to display names
        row_trend_melted['Row_Metric'] = row_trend_melted['Row_Metric_Raw'].map(row_change_trend_cols_display_map)

        fig_row_trend = px.area(
            row_trend_melted,
            x='Date',
            y='Count',
            color='Row_Metric',
            title='Daily Row Changes Over Time',
            labels={'Count': 'Number of Rows/Partitions', 'Row_Metric': 'Change Type'}
        )
        fig_row_trend.update_layout(hovermode="x unified") # Unified hover for daily totals

        use_log_scale_row_trend = st.checkbox("Log Scale Y-axis (Row Changes Trend)", key="log_scale_row_changes_trend_dt_state")
        if use_log_scale_row_trend:
            fig_row_trend.update_yaxes(type='log', rangemode='tozero')
            st.info("Logarithmic scale applied to Y-axis. Zero counts are handled gracefully.", icon="ℹ️")

        st.plotly_chart(fig_row_trend, use_container_width=True)
    else:
        st.info("No row change data (NUMINSERTEDROWS, etc.) available for trend analysis.", icon="ℹ️")

    st.divider()

    # --- Performance & Efficiency Analysis ---
    st.subheader("Performance & Efficiency Analysis")
    chart_cols_state_row2 = st.columns([1, 1])

    with chart_cols_state_row2[0]: # Refresh Duration Trend (Line Chart with Averages)
        st.markdown("<p style='font-size:16px;'><b>Refresh Duration Trend</b></p>", unsafe_allow_html=True)
        st.write("Average and Median refresh duration over time.")
        if 'REFRESH_DURATION_SEC' in filtered_history_df.columns and not filtered_history_df['REFRESH_DURATION_SEC'].empty:
            duration_trend_df = filtered_history_df.groupby(
                pd.Grouper(key='DATA_TIMESTAMP_DT', freq='D')
            )['REFRESH_DURATION_SEC'].agg(['mean', 'median', lambda x: x.quantile(0.95)]).reset_index()
            duration_trend_df.columns = ['Date', 'Mean Duration', 'Median Duration', 'P95 Duration']

            fig_duration_trend = px.line(
                duration_trend_df,
                x='Date',
                y=['Mean Duration', 'Median Duration', 'P95 Duration'],
                title='Daily Refresh Duration Trend (Mean, Median, P95)',
                labels={'value': 'Duration (seconds)'} # Default label
            )

            use_log_scale_duration_trend = st.checkbox("Log Scale Y-axis (Duration Trend)", key="log_scale_duration_trend_dt_state")
            if use_log_scale_duration_trend:
                fig_duration_trend.update_yaxes(type='log', title='Duration (Log Scale)', rangemode='tozero')
                st.info("Logarithmic scale applied to Y-axis.", icon="ℹ️")

            st.plotly_chart(fig_duration_trend, use_container_width=True)
        else:
            st.info("No REFRESH_DURATION_SEC data available for trend analysis.", icon="ℹ️")


    with chart_cols_state_row2[1]: # Refresh Duration vs. Rows Processed (Scatter Plot)
        st.markdown("<p style='font-size:16px;'><b>Refresh Duration vs. Rows Processed</b></p>", unsafe_allow_html=True)
        st.write("Examine if duration correlates with data volume.")
        
        # Define all possible row change columns from DDL for selection
        all_row_metrics_scatter = {
            'Total Rows Changed': ['NUMINSERTEDROWS', 'NUMDELETEDROWS', 'NUMCOPIEDROWS', 'NUMADDEDPARTITIONS', 'NUMREMOVEDPARTITIONS'],
            'Inserted Rows': ['NUMINSERTEDROWS'],
            'Deleted Rows': ['NUMDELETEDROWS'],
            'Copied Rows': ['NUMCOPIEDROWS'],
            'Added Partitions': ['NUMADDEDPARTITIONS'],
            'Removed Partitions': ['NUMREMOVEDPARTITIONS']
        }
        
        # Filter for metrics where all underlying columns are present in the dataframe
        available_row_metrics_scatter = {
            label: cols for label, cols in all_row_metrics_scatter.items()
            if all(c in filtered_history_df.columns for c in cols)
        }

        if 'REFRESH_DURATION_SEC' in filtered_history_df.columns and available_row_metrics_scatter:
            selected_row_metric_for_scatter = st.selectbox(
                "Select X-axis Row Metric:",
                options=list(available_row_metrics_scatter.keys()),
                key="scatter_x_axis_metric"
            )

            cols_to_sum = available_row_metrics_scatter[selected_row_metric_for_scatter]
            scatter_df = filtered_history_df.copy()
            scatter_df['X_AXIS_METRIC_VALUE'] = scatter_df[cols_to_sum].sum(axis=1)

            scatter_df = scatter_df.dropna(subset=['REFRESH_DURATION_SEC', 'X_AXIS_METRIC_VALUE']).copy()
            
            # Remove points where either X or Y is zero if log scale might be applied
            scatter_df = scatter_df[
                (scatter_df['REFRESH_DURATION_SEC'] > 0) &
                (scatter_df['X_AXIS_METRIC_VALUE'] > 0)
            ].copy()

            if not scatter_df.empty:
                # Format timestamps and duration for hover text
                scatter_df['REFRESH_START_TIME_FMT'] = scatter_df['REFRESH_START_TIME_DT'].dt.strftime('%Y-%m-%d %H:%M:%S')
                scatter_df['REFRESH_END_TIME_FMT'] = scatter_df['REFRESH_END_TIME_DT'].dt.strftime('%Y-%m-%d %H:%M:%S')
                scatter_df['REFRESH_DURATION_FMT_HOVER'] = format_seconds_to_readable(scatter_df['REFRESH_DURATION_SEC'], time_format_option)
                scatter_df['X_AXIS_METRIC_FMT_HOVER'] = scatter_df['X_AXIS_METRIC_VALUE'].apply(lambda x: f"{int(x):,}")

                fig_duration_vs_rows = px.scatter(
                    scatter_df,
                    x='X_AXIS_METRIC_VALUE',
                    y='REFRESH_DURATION_SEC',
                    color='STATE',
                    hover_name='QUALIFIED_NAME',
                    title=f'Refresh Duration vs. {selected_row_metric_for_scatter}',
                    labels={
                        'REFRESH_DURATION_SEC': 'Duration (seconds)',
                        'X_AXIS_METRIC_VALUE': selected_row_metric_for_scatter
                    }
                )

                # Custom hovertemplate for scatter plot
                fig_duration_vs_rows.update_traces(
                    hovertemplate=(
                        '<b>Table: %{hover_name}</b><br>'
                        'State: %{marker.color}<br>' # This correctly gets the state string from color mapping
                        f'{selected_row_metric_for_scatter}: %{{customdata[0]}}<br>' # Corrected customdata index
                        'Duration: %{customdata[1]}<br>' # Corrected customdata index
                        'Action: %{customdata[2]}<br>'
                        'Start: %{customdata[3]}<br>'
                        'End: %{customdata[4]}<br>'
                        'Message: %{customdata[5]}<br>'
                        '<extra></extra>'
                    ),
                    customdata=np.stack((
                        scatter_df['X_AXIS_METRIC_FMT_HOVER'],
                        scatter_df['REFRESH_DURATION_FMT_HOVER'],
                        scatter_df['REFRESH_ACTION'],
                        scatter_df['REFRESH_START_TIME_FMT'],
                        scatter_df['REFRESH_END_TIME_FMT'],
                        scatter_df['STATE_MESSAGE']
                    ), axis=-1)
                )

                # Log scale checkboxes (repositioned)
                col_scatter_log1, col_scatter_log2 = st.columns(2)
                with col_scatter_log1:
                    log_x_scatter = st.checkbox("Log Scale X-axis (Rows Processed)", key="log_x_scatter_dt_state")
                with col_scatter_log2:
                    log_y_scatter = st.checkbox("Log Scale Y-axis (Duration Scatter)", key="log_y_scatter_dt_state")

                if log_x_scatter:
                    fig_duration_vs_rows.update_xaxes(type='log')
                if log_y_scatter:
                    fig_duration_vs_rows.update_yaxes(type='log')

                if log_x_scatter or log_y_scatter:
                    st.info("Logarithmic scale applied to selected axis/axes. Data points with zero values are excluded.", icon="ℹ️")

                st.plotly_chart(fig_duration_vs_rows, use_container_width=True)
            else:
                st.info(f"No valid data for Refresh Duration vs. {selected_row_metric_for_scatter} scatter plot after filters (considering log scale requirements).", icon="ℹ️")
        else:
            st.info("Necessary columns for Refresh Duration vs. Rows Processed analysis not available or no data after filters.", icon="ℹ️")
    
    st.divider()

    chart_cols_state_row3 = st.columns([1, 1])

    with chart_cols_state_row3[0]: # Refresh Duration Distribution (Histogram)
        st.markdown("<p style='font-size:16px;'><b>Refresh Duration Distribution</b></p>", unsafe_allow_html=True)
        st.write("Distribution of refresh durations.")
        if 'REFRESH_DURATION_SEC' in filtered_history_df.columns and not filtered_history_df['REFRESH_DURATION_SEC'].empty:
            duration_data = filtered_history_df['REFRESH_DURATION_SEC'].dropna().copy()
            
            # Max duration for slider/histogram range
            max_duration = duration_data.max() if not duration_data.empty else 60

            duration_max_limit_sec = st.slider(
                f"Max Duration for Histogram (seconds):",
                min_value=0.0,
                max_value=float(max_duration),
                value=min(float(max_duration), 3600.0), # Default to 1 hour or max if less
                step=10.0,
                format="%.0f s",
                key="duration_hist_limit"
            )

            # Filter data for histogram based on slider limit
            duration_data_for_hist = duration_data[duration_data <= duration_max_limit_sec].copy()
            
            # Ensure no zero values if log scale is to be applied, replace with tiny positive
            if st.session_state.get("log_scale_duration_hist_dt_state", False):
                 duration_data_for_hist = duration_data_for_hist.apply(lambda x: x if x > 0 else 1e-9)


            if not duration_data_for_hist.empty:
                fig_duration_hist = px.histogram(
                    duration_data_for_hist,
                    x="REFRESH_DURATION_SEC",
                    nbins=20,
                    title='Refresh Duration Distribution',
                    labels={'REFRESH_DURATION_SEC': f'Duration ({time_format_option})'}, # Dynamic label based on format
                    text_auto=False # Don't show text on bars by default
                )
                fig_duration_hist.update_layout(bargap=0.1)

                use_log_scale_duration_hist = st.checkbox("Log Scale X-axis (Duration Histogram)", key="log_scale_duration_hist_dt_state")
                if use_log_scale_duration_hist:
                    fig_duration_hist.update_xaxes(type='log', rangemode='tozero')
                    st.info("Logarithmic scale applied to X-axis. Zero counts are handled gracefully.", icon="ℹ️")
                
                # Dynamic x-axis tick formatting based on selected time format
                if time_format_option != 'seconds': # Custom formatting for non-seconds
                    # Create custom tick text based on conversion for the main axis
                    # Need to determine appropriate tick values for conversion to avoid cluttered axis
                    # Example: generate 5 ticks between min and max duration
                    if fig_duration_hist.layout.xaxis.range:
                        # Get actual range if chart is already drawn
                        min_x_val = fig_duration_hist.layout.xaxis.range[0]
                        max_x_val = fig_duration_hist.layout.xaxis.range[1]
                    else:
                        min_x_val = duration_data_for_hist.min()
                        max_x_val = duration_data_for_hist.max()
                    
                    if min_x_val < 1e-9: # Handle very small numbers for log scale
                        min_x_val = 1.0 # Or some appropriate minimum for visualization
                    
                    # For log scale, ticks should be powers of 10 or similar.
                    # For linear, just simple linspace.
                    # This logic assumes linear range unless log is active.
                    if use_log_scale_duration_hist:
                        # For log scale, Plotly usually handles tick text well for common log bases.
                        # If needed, manually calculate log ticks here.
                        pass # Rely on Plotly's default log tick text
                    else:
                        # For linear, use linspace and format
                        num_ticks = 5
                        if max_x_val > 0 and num_ticks > 0: # Avoid division by zero or too many ticks
                            tick_values = np.linspace(min_x_val, max_x_val, num_ticks)
                            tick_texts = format_seconds_to_readable(pd.Series(tick_values), time_format_option).tolist()
                            fig_duration_hist.update_xaxes(tickvals=tick_values, ticktext=tick_texts)
                            
                st.plotly_chart(fig_duration_hist, use_container_width=True)
            else:
                st.info("No refresh duration data within selected range for histogram.", icon="ℹ️")
        else:
            st.info("No REFRESH_DURATION_SEC data available.", icon="ℹ️")


    with chart_cols_state_row3[1]: # Row Change Overview (Grouped Bar Chart)
        st.markdown("<p style='font-size:16px;'><b>Row Change Overview</b></p>", unsafe_allow_html=True)
        st.write("Total inserted, deleted, and copied rows per refresh action.")
        row_change_analysis_cols = ['NUMINSERTEDROWS', 'NUMDELETEDROWS', 'NUMCOPIEDROWS', 'NUMADDEDPARTITIONS', 'NUMREMOVEDPARTITIONS']
        
        # Filter for columns that actually exist in the DataFrame
        available_row_analysis_cols = [col for col in row_change_analysis_cols if col in filtered_history_df.columns]

        if available_row_analysis_cols and not filtered_history_df.empty:
            row_change_summary = filtered_history_df.groupby('REFRESH_ACTION')[available_row_analysis_cols].sum().reset_index()

            if not row_change_summary.empty:
                row_change_melted = row_change_summary.melt(
                    id_vars='REFRESH_ACTION',
                    value_vars=available_row_analysis_cols,
                    var_name='Row_Metric',
                    value_name='Count'
                )
                fig_row_change = px.bar(
                    row_change_melted,
                    x='REFRESH_ACTION',
                    y='Count',
                    color='Row_Metric',
                    title='Row Changes by Refresh Action',
                    barmode='group',
                    labels={'REFRESH_ACTION': 'Refresh Action', 'Count': 'Number of Rows'}
                )

                use_log_scale_row_change = st.checkbox("Log Scale Y-axis (Row Changes)", key="log_scale_row_change_dt_state")
                if use_log_scale_row_change:
                    fig_row_change.update_yaxes(type='log', rangemode='tozero')
                    st.info("Logarithmic scale applied to Y-axis.", icon="ℹ️")

                st.plotly_chart(fig_row_change, use_container_width=True)
            else:
                st.info("No row change data after aggregation.", icon="ℹ️")
        else:
            st.info("Row change statistics (NUMINSERTEDROWS, etc.) not available or no data after filters.", icon="ℹ️")
    
    st.divider()

    # --- Failure & Anomaly Detection ---
    st.subheader("Failure & Anomaly Detection")

    chart_cols_state_row4 = st.columns([1, 1])

    with chart_cols_state_row4[0]: # Daily / Hourly Failure Count Trend
        st.markdown("<p style='font-size:16px;'><b>Daily Failure Count Trend</b></p>", unsafe_allow_html=True)
        st.write("Count of failed/cancelled refreshes over time.")
        failed_trend_df = filtered_history_df[
            filtered_history_df['STATE'].isin(['FAILED', 'CANCELLED', 'UPSTREAM_FAILED'])
        ].copy()
        if not failed_trend_df.empty:
            failure_counts_daily = failed_trend_df.groupby(
                pd.Grouper(key='DATA_TIMESTAMP_DT', freq='D')
            ).size().reset_index(name='Failure Count')
            failure_counts_daily.rename(columns={'DATA_TIMESTAMP_DT': 'Date'}, inplace=True)

            fig_failure_trend = px.line(
                failure_counts_daily,
                x='Date',
                y='Failure Count',
                title='Daily Failed/Cancelled Refreshes',
                labels={'Failure Count': 'Number of Failures'}
            )

            use_log_scale_failure_trend = st.checkbox("Log Scale Y-axis (Failure Trend)", key="log_scale_failure_trend_dt_state")
            if use_log_scale_failure_trend:
                fig_failure_trend.update_yaxes(type='log', rangemode='tozero')
                st.info("Logarithmic scale applied to Y-axis.", icon="ℹ️")

            st.plotly_chart(fig_failure_trend, use_container_width=True)
        else:
            st.info("No failed or cancelled refreshes to trend.", icon="ℹ️")

    with chart_cols_state_row4[1]: # Refresh Trigger Distribution
        st.markdown("<p style='font-size:16px;'><b>Refresh Trigger Distribution</b></p>", unsafe_allow_html=True)
        st.write("Distribution of refresh triggers (e.g., SCHEDULED, MANUAL).")
        if 'REFRESH_TRIGGER' in filtered_history_df.columns and not filtered_history_df.empty:
            trigger_counts = filtered_history_df['REFRESH_TRIGGER'].value_counts().reset_index()
            trigger_counts.columns = ['Trigger', 'Count']
            fig_trigger_dist = px.pie(
                trigger_counts,
                values='Count',
                names='Trigger',
                title='Refresh Trigger Distribution',
                hole=0.3
            )
            fig_trigger_dist.update_traces(textposition='inside', textinfo='percent+label')
            st.plotly_chart(fig_trigger_dist, use_container_width=True)
        else:
            st.info("No REFRESH_TRIGGER data available.", icon="ℹ️")
    
    st.divider()

    # --- Detailed Failed Refreshes Table ---
    st.subheader("Failed & Cancelled Refreshes Details")
    st.write("Detailed list of dynamic table refreshes that did not succeed.")

    failed_refreshes_df = filtered_history_df[
        filtered_history_df['STATE'].isin(['FAILED', 'CANCELLED', 'UPSTREAM_FAILED'])
    ].copy()

    if not failed_refreshes_df.empty:
        # Checkbox for "Show latest per table"
        show_latest_failed_per_table = st.checkbox("Show only the latest failed/cancelled refresh per table", key="latest_failed_per_table")

        if show_latest_failed_per_table:
            # Sort by QUALIFIED_NAME and then by DATA_TIMESTAMP_DT (descending)
            # Then group by QUALIFIED_NAME and take the first (most recent)
            failed_refreshes_to_display = failed_refreshes_df.sort_values(
                by=['QUALIFIED_NAME', 'DATA_TIMESTAMP_DT'], ascending=[True, False]
            ).groupby('QUALIFIED_NAME').first().reset_index()
            st.info("Displaying the latest failed/cancelled refresh for each affected table.", icon="ℹ️")
        else:
            failed_refreshes_to_display = failed_refreshes_df.sort_values('DATA_TIMESTAMP_DT', ascending=False)
        

        display_cols = [
            'QUALIFIED_NAME', 'STATE', 'STATE_CODE', 'STATE_MESSAGE', 'QUERY_ID',
            'DATA_TIMESTAMP_DT', 'REFRESH_START_TIME_DT', 'REFRESH_END_TIME_DT',
            'REFRESH_DURATION_SEC', 'REFRESH_ACTION', 'REFRESH_TRIGGER'
        ]
        
        if 'REFRESH_DURATION_SEC' in failed_refreshes_to_display.columns:
            failed_refreshes_to_display['REFRESH_DURATION_FMT'] = format_seconds_to_readable(
                failed_refreshes_to_display['REFRESH_DURATION_SEC'], time_format_option
            )
            display_cols = [col if col != 'REFRESH_DURATION_SEC' else 'REFRESH_DURATION_FMT' for col in display_cols]


        display_labels = {
            'QUALIFIED_NAME': 'Dynamic Table',
            'STATE': 'State',
            'STATE_CODE': 'Code',
            'STATE_MESSAGE': 'Message',
            'QUERY_ID': 'Query ID',
            'DATA_TIMESTAMP_DT': 'Data Timestamp',
            'REFRESH_START_TIME_DT': 'Refresh Start',
            'REFRESH_END_TIME_DT': 'Refresh End',
            'REFRESH_DURATION_FMT': 'Duration',
            'REFRESH_ACTION': 'Action',
            'REFRESH_TRIGGER': 'Trigger'
        }

        final_display_df = failed_refreshes_to_display[
            [col for col in display_cols if col in failed_refreshes_to_display.columns]
        ].rename(columns=display_labels)

        st.dataframe(final_display_df, use_container_width=True)
    else:
        st.info("No failed or cancelled refreshes found based on current filters. Great news!", icon="🎉")

    st.divider()

    # --- Top N Longest Refreshes Table ---
    st.subheader("Top Longest Refreshes")
    st.write("Identifies historical refresh events with the longest durations.")

    longest_refreshes_df = filtered_history_df[
        (filtered_history_df['STATE'] == 'SUCCEEDED') &
        (filtered_history_df['REFRESH_DURATION_SEC'].notna())
    ].copy()

    if not longest_refreshes_df.empty:
        # Checkbox for "Show longest per table"
        show_longest_per_table = st.checkbox("Show only the longest refresh per table", key="longest_refresh_per_table")

        if show_longest_per_table:
            # Sort by QUALIFIED_NAME and then by REFRESH_DURATION_SEC (descending)
            # Then group by QUALIFIED_NAME and take the first (longest)
            longest_refreshes_to_display = longest_refreshes_df.sort_values(
                by=['QUALIFIED_NAME', 'REFRESH_DURATION_SEC'], ascending=[True, False]
            ).groupby('QUALIFIED_NAME').first().reset_index()
            st.info("Displaying the longest refresh for each successful table.", icon="ℹ️")
        else:
            longest_refreshes_to_display = longest_refreshes_df.sort_values('REFRESH_DURATION_SEC', ascending=False).head(10)
            
        
        longest_refreshes_to_display['REFRESH_DURATION_FMT'] = format_seconds_to_readable(
            longest_refreshes_to_display['REFRESH_DURATION_SEC'], time_format_option
        )

        display_cols_longest = [
            'QUALIFIED_NAME', 'REFRESH_DURATION_FMT', 'REFRESH_ACTION', 'REFRESH_TRIGGER',
            'REFRESH_START_TIME_DT', 'REFRESH_END_TIME_DT',
            'NUMINSERTEDROWS', 'NUMDELETEDROWS', 'NUMCOPIEDROWS', 'NUMADDEDPARTITIONS', 'NUMREMOVEDPARTITIONS' # Added these
        ]

        display_labels_longest = {
            'QUALIFIED_NAME': 'Dynamic Table',
            'REFRESH_DURATION_FMT': 'Duration',
            'REFRESH_ACTION': 'Action',
            'REFRESH_TRIGGER': 'Trigger',
            'REFRESH_START_TIME_DT': 'Start Time',
            'REFRESH_END_TIME_DT': 'End Time',
            'NUMINSERTEDROWS': 'Ins. Rows',
            'NUMDELETEDROWS': 'Del. Rows',
            'NUMCOPIEDROWS': 'Copied Rows',
            'NUMADDEDPARTITIONS': 'Added Part.',
            'NUMREMOVEDPARTITIONS': 'Rem. Part.'
        }

        final_display_longest_df = longest_refreshes_to_display[
            [col for col in display_cols_longest if col in longest_refreshes_to_display.columns]
        ].rename(columns=display_labels_longest)

        st.dataframe(final_display_longest_df, use_container_width=True)
    else:
        st.info("No successful refreshes found to identify longest durations.", icon="ℹ️")
    
    st.divider()

    # --- Top N Frequent Failure Messages / Codes Table ---
    st.subheader("Frequent Failure Patterns")
    st.write("Identifies recurring error messages or codes from failed refreshes.")

    failure_patterns_df = filtered_history_df[
        filtered_history_df['STATE'].isin(['FAILED', 'CANCELLED', 'UPSTREAM_FAILED'])
    ].copy()

    if not failure_patterns_df.empty:
        # Group by STATE_CODE and STATE_MESSAGE, count occurrences, and get the last occurrence time
        failure_summary = failure_patterns_df.groupby(['STATE_CODE', 'STATE_MESSAGE']).agg(
            Occurrence_Count=('STATE_CODE', 'size'), # Count occurrences
            Last_Occurred_Time=('DATA_TIMESTAMP_DT', 'max') # Get latest timestamp
        ).reset_index()
        
        failure_summary = failure_summary.sort_values('Occurrence_Count', ascending=False)

        # Select columns and rename for display
        display_cols_failure = ['STATE_CODE', 'STATE_MESSAGE', 'Occurrence_Count', 'Last_Occurred_Time']
        display_labels_failure = {
            'STATE_CODE': 'Error Code',
            'STATE_MESSAGE': 'Error Message',
            'Occurrence_Count': 'Count',
            'Last_Occurred_Time': 'Last Occurred'
        }

        final_display_failure_df = failure_summary[
            [col for col in display_cols_failure if col in failure_summary.columns]
        ].rename(columns=display_labels_failure)

        st.dataframe(final_display_failure_df, use_container_width=True)
    else:
        st.info("No failed or cancelled refreshes to analyze for frequent patterns.", icon="ℹ️")
