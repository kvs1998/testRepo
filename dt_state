# tabs/dt_state_tab.py
import streamlit as st
import pandas as pd
import plotly.express as px
import numpy as np
from datetime import datetime, time, date


# Helper function to convert seconds to a more readable format
def format_seconds_to_readable(seconds_series, format_type):
    if format_type == "seconds":
        return seconds_series.round(1).astype(str) + "s"
    elif format_type == "minutes":
        return (seconds_series / 60).round(1).astype(str) + "m"
    elif format_type == "hours":
        return (seconds_series / 3600).round(1).astype(str) + "h"
    elif format_type == "days":
        return (seconds_series / 86400).round(1).astype(str) + "d"
    elif format_type == "mixed":
        def mix_format(s):
            if pd.isna(s) or s is None: return "N/A"
            s = float(s)
            if s == 0: return "0s"

            days = int(s // 86400)
            hours = int((s % 86400) // 3600)
            minutes = int((s % 3600) // 60)
            seconds = s % 60

            parts = []
            if days > 0: parts.append(f"{days}d")
            if hours > 0: parts.append(f"{hours}h")
            if minutes > 0: parts.append(f"{minutes}m")
            if seconds > 0 and (not parts or seconds >= 1):
                parts.append(f"{seconds:.1f}s")

            return " ".join(parts) if parts else "0s"
        return seconds_series.apply(mix_format)
    return seconds_series


def render_dt_state_tab(history_df: pd.DataFrame):
    st.header("Dynamic Table Refresh State & History")
    st.write("Analyze the refresh patterns, durations, and outcomes of your dynamic tables over time.")

    if history_df.empty:
        st.info("No refresh history data available. Check data source or collection.", icon="ℹ️")
        return

    # --- Data Preprocessing ---
    for col in ['REFRESH_START_TIME', 'REFRESH_END_TIME', 'DATA_TIMESTAMP']:
        if col in history_df.columns:
            history_df[f'{col}_DT'] = pd.to_datetime(history_df[col], errors='coerce')

    if 'REFRESH_START_TIME_DT' in history_df.columns and 'REFRESH_END_TIME_DT' in history_df.columns:
        history_df['REFRESH_DURATION_SEC'] = (
            history_df['REFRESH_END_TIME_DT'] - history_df['REFRESH_START_TIME_DT']
        ).dt.total_seconds().fillna(0)

    row_change_raw_cols = ['NUMINSERTEDROWS', 'NUMDELETEDROWS', 'NUMCOPIEDROWS', 'NUMADDEDPARTITIONS', 'NUMREMOVEDPARTITIONS']
    for col in row_change_raw_cols:
        if col in history_df.columns:
            history_df[col] = pd.to_numeric(history_df[col], errors='coerce').fillna(0)


    # --- Filters for DT State Tab ---
    st.markdown("---")
    st.subheader("Apply Filters for Refresh History")

    filter_cols_dt_state_row1 = st.columns([1, 1, 1, 1])

    with filter_cols_dt_state_row1[0]:
        all_databases_dt_state = ['All'] + sorted(history_df['DATABASE_NAME'].unique().tolist())
        selected_database_dt_state = st.selectbox(
            "Database:", options=all_databases_dt_state, key="db_filter_dt_state"
        )
    with filter_cols_dt_state_row1[1]:
        temp_df_for_schema_options = history_df.copy()
        if selected_database_dt_state != 'All':
            temp_df_for_schema_options = temp_df_for_schema_options[temp_df_for_schema_options['DATABASE_NAME'] == selected_database_dt_state]

        if not temp_df_for_schema_options.empty:
            schemas_in_db_dt_state = ['All'] + sorted(temp_df_for_schema_options['SCHEMA_NAME'].unique().tolist())
        else:
            schemas_in_db_dt_state = ['All']
            st.info("No schemas found for selected Database.", icon="ℹ️")

        selected_schema_dt_state = st.selectbox(
            "Schema:", options=schemas_in_db_dt_state, key="schema_filter_dt_state"
        )

    df_for_table_options_state = history_df.copy()
    if selected_database_dt_state != 'All':
        df_for_table_options_state = df_for_table_options_state[df_for_table_options_state['DATABASE_NAME'] == selected_database_dt_state]
    if selected_schema_dt_state != 'All':
        df_for_table_options_state = df_for_table_options_state[df_for_table_options_state['SCHEMA_NAME'] == selected_schema_dt_state]


    with filter_cols_dt_state_row1[2]:
        if not df_for_table_options_state.empty:
            all_tables_dt_state_options = ['All'] + sorted(df_for_table_options_state['TABLE_NAME'].unique().tolist())
            default_tables_dt_state_selected = ['All']
        else:
            all_tables_dt_state_options = ['All']
            default_tables_dt_state_selected = ['All']
            st.info("No tables found for selected DB/Schema.", icon="ℹ️")
        selected_table_dt_state = st.multiselect(
            "Table(s):", options=all_tables_dt_state_options, default=default_tables_dt_state_selected, key="table_filter_dt_state"
        )

    with filter_cols_dt_state_row1[3]:
        all_states = ['All'] + sorted(history_df['STATE'].unique().tolist())
        selected_state = st.multiselect(
            "Refresh State(s):", options=all_states, default=['All'], key="refresh_state_filter_dt_state"
        )

    # New filter row for Date Range, Duration Format, and Target Lag
    filter_cols_dt_state_row2 = st.columns([1, 1, 1]) # Added a column for Target Lag

    with filter_cols_dt_state_row2[0]:
        # Robust date range handling
        min_data_date = history_df['DATA_TIMESTAMP_DT'].min().date() if not history_df.empty else datetime.now().date()
        max_data_date = history_df['DATA_TIMESTAMP_DT'].max().date() if not history_df.empty else datetime.now().date()
        
        # Determine initial value for date_input. Ensure it's a tuple of 2 dates.
        # Handle cases where min_data_date == max_data_date
        if min_data_date == max_data_date:
            initial_date_tuple = (min_data_date, max_data_date)
        else:
            initial_date_tuple = (min_data_date, max_data_date)

        selected_date_input = st.date_input(
            "Select Date Range for History:",
            value=initial_date_tuple, # Initial value as a tuple
            min_value=min_data_date,
            max_value=max_data_date,
            key="date_range_dt_state"
        )
        
        # Parse selected_date_input to define start_date_filter and end_date_filter Timestamps
        # This handles tuple output (range) or single date object output from st.date_input
        start_date_filter = None
        end_date_filter = None

        if isinstance(selected_date_input, tuple):
            # Handle (date1, date2)
            if len(selected_date_input) == 2:
                start_date_filter = pd.to_datetime(selected_date_input[0]).normalize()
                end_date_filter = pd.to_datetime(selected_date_input[1]).normalize() + pd.Timedelta(days=1) - pd.Timedelta(microseconds=1)
            # Handle (date, None) or (None, date) or empty tuple if user cleared one side
            elif len(selected_date_input) == 1 and selected_date_input[0] is not None:
                start_date_filter = pd.to_datetime(selected_date_input[0]).normalize()
                end_date_filter = history_df['DATA_TIMESTAMP_DT'].max().normalize() + pd.Timedelta(days=1) - pd.Timedelta(microseconds=1)
            elif len(selected_date_input) == 0 or selected_date_input[0] is None: # Both empty or (None,)
                # Default to full data range
                start_date_filter = history_df['DATA_TIMESTAMP_DT'].min()
                end_date_filter = history_df['DATA_TIMESTAMP_DT'].max()
        elif isinstance(selected_date_input, (datetime.date, datetime)): # Single date selected
            start_date_filter = pd.to_datetime(selected_date_input).normalize()
            end_date_filter = pd.to_datetime(selected_date_input).normalize() + pd.Timedelta(days=1) - pd.Timedelta(microseconds=1)
        else: # Fallback to full range if input is unexpected
            start_date_filter = history_df['DATA_TIMESTAMP_DT'].min()
            end_date_filter = history_df['DATA_TIMESTAMP_DT'].max()


    with filter_cols_dt_state_row2[1]:
        time_format_option = st.radio(
            "Display Durations In:",
            options=["mixed", "seconds", "minutes", "hours", "days"],
            index=0,
            horizontal=True,
            key="duration_format_dt_state"
        )
    
    with filter_cols_dt_state_row2[2]: # Target Lag Filter (Selectbox) - NEW
        target_lag_options_display = ['All']
        target_lag_value_map = {}

        if 'TARGET_LAG_SEC' in history_df.columns and not history_df['TARGET_LAG_SEC'].empty:
            unique_target_lags_sec = sorted(history_df['TARGET_LAG_SEC'].dropna().astype(float).unique().tolist())
            for sec_val in unique_target_lags_sec:
                formatted_val = format_seconds_to_readable(pd.Series([sec_val]), 'mixed').iloc[0]
                target_lag_options_display.append(formatted_val)
                target_lag_value_map[formatted_val] = sec_val
        else:
            st.info("No 'TARGET_LAG_SEC' data to populate target lag filter.", icon="ℹ️")

        selected_target_lag_display = st.selectbox(
            "Target Lag:",
            options=target_lag_options_display,
            key="target_lag_filter_dt_state_selectbox"
        )
        selected_target_lag_sec_for_filter = None
        if selected_target_lag_display != 'All':
            selected_target_lag_sec_for_filter = target_lag_value_map.get(selected_target_lag_display)
            
    st.markdown("---")

    # --- Apply Filters to Main DataFrame ---
    filtered_history_df = history_df.copy()

    if selected_database_dt_state != 'All':
        filtered_history_df = filtered_history_df[filtered_history_df['DATABASE_NAME'] == selected_database_dt_state].copy()
    if selected_schema_dt_state != 'All':
        filtered_history_df = filtered_history_df[filtered_history_df['SCHEMA_NAME'] == selected_schema_dt_state].copy()
    if selected_table_dt_state and 'All' not in selected_table_dt_state:
        filtered_history_df = filtered_history_df[filtered_history_df['TABLE_NAME'].isin(selected_table_dt_state)].copy()
    elif not selected_table_dt_state:
        st.warning("No table(s) selected. Display will be empty.", icon="⚠️")
        filtered_history_df = pd.DataFrame()

    if selected_state and 'All' not in selected_state:
        filtered_history_df = filtered_history_df[filtered_history_df['STATE'].isin(selected_state)].copy()
    elif not selected_state:
        st.warning("No refresh state(s) selected. Display will be empty.", icon="⚠️")
        filtered_history_df = pd.DataFrame()

    # Apply date range filter using Timestamps directly
    if 'DATA_TIMESTAMP_DT' in filtered_history_df.columns:
        filtered_history_df = filtered_history_df[
            (filtered_history_df['DATA_TIMESTAMP_DT'] >= start_date_filter) &
            (filtered_history_df['DATA_TIMESTAMP_DT'] <= end_date_filter)
        ].copy()

    # Apply Target Lag filter (using selectbox value)
    if selected_target_lag_display != 'All' and selected_target_lag_sec_for_filter is not None:
        if 'TARGET_LAG_SEC' in filtered_history_df.columns:
            filtered_history_df = filtered_history_df[
                filtered_history_df['TARGET_LAG_SEC'] == selected_target_lag_sec_for_filter
            ].copy()
        else:
            st.warning("`TARGET_LAG_SEC` column not found in metadata. Target Lag filter skipped.", icon="⚠️")


    if filtered_history_df.empty:
        st.info("No data available based on current filter selections. Please adjust your filters.", icon="ℹ️")
        return

    # --- Refresh Trends & Distributions ---
    st.subheader("Refresh Trends & Status Over Time")
    
    # Chart 1: Daily Refresh Status Trend
    cols_chart1_header = st.columns([0.8, 0.2])
    with cols_chart1_header[0]:
        st.markdown("<p style='font-size:16px;'><b>Daily Refresh Status Trend</b></p>", unsafe_allow_html=True)
    with cols_chart1_header[1]:
        use_log_scale_refresh_status = st.checkbox("Log Scale Y-axis", key="log_scale_refresh_status_dt_state")
    st.write("Count of refreshes by state over the selected time range.")

    chart_cols_state_row1 = st.columns([1, 1])

    with chart_cols_state_row1[0]:
        if not filtered_history_df.empty:
            status_trend_df = filtered_history_df.groupby([
                pd.Grouper(key='DATA_TIMESTAMP_DT', freq='D'), 'STATE'
            ]).size().unstack(fill_value=0).reset_index()
            status_trend_df.rename(columns={'DATA_TIMESTAMP_DT': 'Date'}, inplace=True)

            state_order = ['SUCCEEDED', 'FAILED', 'UPSTREAM_FAILED', 'CANCELLED', 'EXECUTING', 'SCHEDULED']
            present_states = [s for s in state_order if s in status_trend_df.columns]

            status_trend_melted = status_trend_df.melt(
                id_vars=['Date'],
                value_vars=present_states,
                var_name='State',
                value_name='Count'
            )

            fig_status_trend = px.bar(
                status_trend_melted,
                x='Date',
                y='Count',
                color='State',
                title='Daily Refresh Status Trend', # Title is redundant here, but kept for px options
                color_discrete_map={
                    'SUCCEEDED': 'green', 'FAILED': 'red', 'UPSTREAM_FAILED': 'darkred',
                    'CANCELLED': 'orange', 'EXECUTING': 'blue', 'SCHEDULED': 'grey'
                }
            )
            fig_status_trend.update_layout(barmode='stack', showlegend=True, title_text='') # Remove title_text to avoid redundancy

            if use_log_scale_refresh_status:
                fig_status_trend.update_yaxes(type='log', title='Count (Log Scale)', rangemode='tozero')
                st.info("Logarithmic scale applied to Y-axis. Zero counts are handled gracefully.", icon="ℹ️")

            st.plotly_chart(fig_status_trend, use_container_width=True)
        else:
            st.info("No data for Refresh Status Trend.", icon="ℹ️")

    with chart_cols_state_row1[1]:
        st.markdown("<p style='font-size:16px;'><b>Refresh Action Distribution</b></p>", unsafe_allow_html=True)
        st.write("Distribution of refresh actions (e.g., INCREMENTAL, FULL, NO_DATA).")
        if 'REFRESH_ACTION' in filtered_history_df.columns and not filtered_history_df.empty:
            action_counts = filtered_history_df['REFRESH_ACTION'].value_counts().reset_index()
            action_counts.columns = ['Action', 'Count']
            fig_action_dist = px.pie(
                action_counts,
                values='Count',
                names='Action',
                title='Refresh Action Distribution',
                hole=0.3
            )
            fig_action_dist.update_traces(textposition='inside', textinfo='percent+label')
            st.plotly_chart(fig_action_dist, use_container_width=True)
        else:
            st.info("No REFRESH_ACTION data available.", icon="ℹ️")

    st.divider()

    # Chart 2: Row Changes Over Time
    cols_chart2_header = st.columns([0.8, 0.2])
    with cols_chart2_header[0]:
        st.markdown("<p style='font-size:16px;'><b>Row Changes Over Time</b></p>", unsafe_allow_html=True)
    with cols_chart2_header[1]:
        use_log_scale_row_trend = st.checkbox("Log Scale Y-axis", key="log_scale_row_changes_trend_dt_state")
    st.write("Trend of inserted, deleted, copied, added partitions, and removed partitions over time.")

    if available_row_trend_cols and not filtered_history_df.empty:
        row_trend_df = filtered_history_df.groupby(pd.Grouper(key='DATA_TIMESTAMP_DT', freq='D'))[available_row_trend_cols].sum().reset_index()
        row_trend_df.rename(columns={'DATA_TIMESTAMP_DT': 'Date'}, inplace=True)

        row_trend_melted = row_trend_df.melt(
            id_vars=['Date'],
            value_vars=available_row_trend_cols,
            var_name='Row_Metric_Raw',
            value_name='Count'
        )
        row_trend_melted['Row_Metric'] = row_trend_melted['Row_Metric_Raw'].map(row_change_trend_cols_display_map)

        fig_row_trend = px.area(
            row_trend_melted,
            x='Date',
            y='Count',
            color='Row_Metric',
            title='Daily Row Changes Over Time',
            labels={'Count': 'Number of Rows/Partitions', 'Row_Metric': 'Change Type'}
        )
        fig_row_trend.update_layout(hovermode="x unified", title_text='') # Remove title_text

        if use_log_scale_row_trend:
            fig_row_trend.update_yaxes(type='log', rangemode='tozero')
            st.info("Logarithmic scale applied to Y-axis. Zero counts are handled gracefully.", icon="ℹ️")

        st.plotly_chart(fig_row_trend, use_container_width=True)
    else:
        st.info("No row change data (NUMINSERTEDROWS, etc.) available for trend analysis.", icon="ℹ️")

    st.divider()

    # --- Performance & Efficiency Analysis ---
    st.subheader("Performance & Efficiency Analysis")
    
    # Chart 3: Refresh Duration Trend
    cols_chart3_header = st.columns([0.8, 0.2])
    with cols_chart3_header[0]:
        st.markdown("<p style='font-size:16px;'><b>Refresh Duration Trend</b></p>", unsafe_allow_html=True)
    with cols_chart3_header[1]:
        use_log_scale_duration_trend = st.checkbox("Log Scale Y-axis", key="log_scale_duration_trend_dt_state")
    st.write("Average and Median refresh duration over time.")

    chart_cols_state_row2 = st.columns([1, 1])

    with chart_cols_state_row2[0]:
        if 'REFRESH_DURATION_SEC' in filtered_history_df.columns and not filtered_history_df['REFRESH_DURATION_SEC'].empty:
            duration_trend_df = filtered_history_df.groupby(
                pd.Grouper(key='DATA_TIMESTAMP_DT', freq='D')
            )['REFRESH_DURATION_SEC'].agg(['mean', 'median', lambda x: x.quantile(0.95)]).reset_index()
            duration_trend_df.columns = ['Date', 'Mean Duration', 'Median Duration', 'P95 Duration']

            fig_duration_trend = px.line(
                duration_trend_df,
                x='Date',
                y=['Mean Duration', 'Median Duration', 'P95 Duration'],
                title='Daily Refresh Duration Trend (Mean, Median, P95)', # Title is redundant
                labels={'value': f'Duration ({time_format_option})'} # Dynamic label
            )
            fig_duration_trend.update_layout(title_text='') # Remove title_text

            if use_log_scale_duration_trend:
                fig_duration_trend.update_yaxes(type='log', title='Duration (Log Scale)', rangemode='tozero')
                st.info("Logarithmic scale applied to Y-axis.", icon="ℹ️")

            st.plotly_chart(fig_duration_trend, use_container_width=True)
        else:
            st.info("No REFRESH_DURATION_SEC data available for trend analysis.", icon="ℹ️")


    with chart_cols_state_row2[1]: # Refresh Duration vs. Rows Processed (Scatter Plot)
        st.markdown("<p style='font-size:16px;'><b>Refresh Duration vs. Rows Processed</b></p>", unsafe_allow_html=True)
        st.write("Examine if duration correlates with data volume.")
        
        all_row_metrics_scatter = {
            'Total Rows Changed': ['NUMINSERTEDROWS', 'NUMDELETEDROWS', 'NUMCOPIEDROWS', 'NUMADDEDPARTITIONS', 'NUMREMOVEDPARTITIONS'],
            'Inserted Rows': ['NUMINSERTEDROWS'],
            'Deleted Rows': ['NUMDELETEDROWS'],
            'Copied Rows': ['NUMCOPIEDROWS'],
            'Added Partitions': ['NUMADDEDPARTITIONS'],
            'Removed Partitions': ['NUMREMOVEDPARTITIONS']
        }
        
        available_row_metrics_scatter = {
            label: cols for label, cols in all_row_metrics_scatter.items()
            if all(c in filtered_history_df.columns for c in cols)
        }

        if 'REFRESH_DURATION_SEC' in filtered_history_df.columns and available_row_metrics_scatter:
            selected_row_metric_for_scatter = st.selectbox(
                "Select X-axis Row Metric:",
                options=list(available_row_metrics_scatter.keys()),
                key="scatter_x_axis_metric"
            )

            cols_to_sum = available_row_metrics_scatter[selected_row_metric_for_scatter]
            scatter_df = filtered_history_df.copy()
            scatter_df['X_AXIS_METRIC_VALUE'] = scatter_df[cols_to_sum].sum(axis=1)

            scatter_df = scatter_df.dropna(subset=['REFRESH_DURATION_SEC', 'X_AXIS_METRIC_VALUE']).copy()
            
            # Log scale checkboxes (repositioned) - moved above scatter plot
            col_scatter_log1, col_scatter_log2 = st.columns(2)
            with col_scatter_log1:
                log_x_scatter = st.checkbox("Log Scale X-axis (Rows Processed)", key="log_x_scatter_dt_state")
            with col_scatter_log2:
                log_y_scatter = st.checkbox("Log Scale Y-axis (Duration Scatter)", key="log_y_scatter_dt_state")

            # Remove points where either X or Y is zero if log scale might be applied
            if log_x_scatter or log_y_scatter:
                scatter_df = scatter_df[
                    (scatter_df['REFRESH_DURATION_SEC'] > 0) &
                    (scatter_df['X_AXIS_METRIC_VALUE'] > 0)
                ].copy()

            if not scatter_df.empty:
                # Format timestamps and duration for hover text
                scatter_df['REFRESH_START_TIME_FMT'] = scatter_df['REFRESH_START_TIME_DT'].dt.strftime('%Y-%m-%d %H:%M:%S')
                scatter_df['REFRESH_END_TIME_FMT'] = scatter_df['REFRESH_END_TIME_DT'].dt.strftime('%Y-%m-%d %H:%M:%S')
                scatter_df['REFRESH_DURATION_FMT_HOVER'] = format_seconds_to_readable(scatter_df['REFRESH_DURATION_SEC'], time_format_option)
                scatter_df['X_AXIS_METRIC_FMT_HOVER'] = scatter_df['X_AXIS_METRIC_VALUE'].apply(lambda x: f"{int(x):,}")


                fig_duration_vs_rows = px.scatter(
                    scatter_df,
                    x='X_AXIS_METRIC_VALUE',
                    y='REFRESH_DURATION_SEC',
                    color='STATE',
                    # No hover_name here, customdata will handle all
                    # hover_data is used to pass additional columns not directly used as x/y/color
                    hover_data={
                        'REFRESH_ACTION': True,
                        'STATE_MESSAGE': True,
                        'QUALIFIED_NAME': False, # Explicitly controlled in customdata
                        'STATE': False, # Explicitly controlled in customdata
                        'REFRESH_START_TIME_DT': False, # Explicitly controlled in customdata
                        'REFRESH_END_TIME_DT': False # Explicitly controlled in customdata
                    },
                    title=f'Refresh Duration vs. {selected_row_metric_for_scatter}',
                    labels={
                        'REFRESH_DURATION_SEC': f'Duration ({time_format_option})', # Dynamic label
                        'X_AXIS_METRIC_VALUE': selected_row_metric_for_scatter
                    },
                    color_discrete_map={ # Explicitly map colors for consistency
                        'SUCCEEDED': 'blue', 'FAILED': 'red', 'UPSTREAM_FAILED': 'darkred',
                        'CANCELLED': 'orange', 'EXECUTING': 'green', 'SCHEDULED': 'grey', 'UNKNOWN': 'purple'
                    }
                )

                # Custom hovertemplate for scatter plot - now correctly showing table name and state
                fig_duration_vs_rows.update_traces(
                    hovertemplate=(
                        '<b>Table: %{customdata[0]}</b><br>' # QUALIFIED_NAME from customdata
                        'State: %{customdata[1]}<br>'       # STATE from customdata
                        f'{selected_row_metric_for_scatter}: %{{customdata[2]}}<br>'
                        'Duration: %{customdata[3]}<br>'
                        'Action: %{customdata[4]}<br>'
                        'Start: %{customdata[5]}<br>'
                        'End: %{customdata[6]}<br>'
                        'Message: %{customdata[7]}<br>'
                        '<extra></extra>' # Hides the default secondary hover info
                    ),
                    customdata=np.stack((
                        scatter_df['QUALIFIED_NAME'],
                        scatter_df['STATE'], # Use raw state string here
                        scatter_df['X_AXIS_METRIC_FMT_HOVER'],
                        scatter_df['REFRESH_DURATION_FMT_HOVER'],
                        scatter_df['REFRESH_ACTION'],
                        scatter_df['REFRESH_START_TIME_FMT'],
                        scatter_df['REFRESH_END_TIME_FMT'],
                        scatter_df['STATE_MESSAGE']
                    ), axis=-1)
                )

                if log_x_scatter:
                    fig_duration_vs_rows.update_xaxes(type='log')
                if log_y_scatter:
                    fig_duration_vs_rows.update_yaxes(type='log')

                if log_x_scatter or log_y_scatter:
                    st.info("Logarithmic scale applied to selected axis/axes. Data points with zero values are excluded.", icon="ℹ️")

                st.plotly_chart(fig_duration_vs_rows, use_container_width=True)
            else:
                st.info(f"No valid data for Refresh Duration vs. {selected_row_metric_for_scatter} scatter plot after filters (considering log scale requirements).", icon="ℹ️")
        else:
            st.info("Necessary columns for Refresh Duration vs. Rows Processed analysis not available or no data after filters.", icon="ℹ️")
    
    st.divider()

    # Chart 5: Refresh Duration Distribution (Histogram)
    cols_chart5_header = st.columns([0.8, 0.2])
    with cols_chart5_header[0]:
        st.markdown("<p style='font-size:16px;'><b>Refresh Duration Distribution</b></p>", unsafe_allow_html=True)
    with cols_chart5_header[1]:
        use_log_scale_duration_hist = st.checkbox("Log Scale X-axis", key="log_scale_duration_hist_dt_state")
    st.write("Distribution of refresh durations.")
    
    chart_cols_state_row3 = st.columns([1]) # This histogram takes full width

    with chart_cols_state_row3[0]: # Refresh Duration Distribution (Histogram)
        if 'REFRESH_DURATION_SEC' in filtered_history_df.columns and not filtered_history_df['REFRESH_DURATION_SEC'].empty:
            duration_data = filtered_history_df['REFRESH_DURATION_SEC'].dropna().copy()
            
            # Ensure no zero values if log scale is to be applied, replace with tiny positive
            if use_log_scale_duration_hist:
                 duration_data = duration_data.apply(lambda x: x if x > 0 else 1e-9)


            if not duration_data.empty:
                fig_duration_hist = px.histogram(
                    duration_data,
                    x="REFRESH_DURATION_SEC",
                    nbins=20, # Removed slider, using a fixed nbins
                    title='Refresh Duration Distribution',
                    labels={'REFRESH_DURATION_SEC': f'Duration ({time_format_option})'},
                    text_auto=False
                )
                fig_duration_hist.update_layout(bargap=0.1, title_text='') # Remove title_text

                if use_log_scale_duration_hist:
                    fig_duration_hist.update_xaxes(type='log', rangemode='tozero')
                    st.info("Logarithmic scale applied to X-axis. Zero counts are handled gracefully.", icon="ℹ️")
                
                # Dynamic x-axis tick formatting based on selected time format
                if time_format_option != 'seconds':
                    # Determine appropriate tick values based on the data range AFTER filtering and log scale.
                    # Use figure's updated range if available, else original data range.
                    min_x_val = fig_duration_hist.layout.xaxis.range[0] if fig_duration_hist.layout.xaxis.range else duration_data.min()
                    max_x_val = fig_duration_hist.layout.xaxis.range[1] if fig_duration_hist.layout.xaxis.range else duration_data.max()
                    
                    if min_x_val < 1e-9 and use_log_scale_duration_hist: # Adjust for log scale if min is near zero
                        min_x_val = 1.0

                    if max_x_val > 0:
                        if not use_log_scale_duration_hist:
                            tick_values = np.linspace(min_x_val, max_x_val, num=5, endpoint=True) # Generate 5 linear ticks
                        else:
                            # For log scale, generate ticks at powers of 10 for better readability
                            log_min = np.floor(np.log10(min_x_val)) if min_x_val > 0 else 0
                            log_max = np.ceil(np.log10(max_x_val))
                            tick_values = np.logspace(log_min, log_max, int(log_max - log_min) + 1)
                            tick_values = tick_values[(tick_values >= min_x_val) & (tick_values <= max_x_val)]
                            if not tick_values.size: # Fallback if no ticks generated
                                tick_values = np.linspace(min_x_val, max_x_val, 3) # Fallback to 3 linear ticks if log generates none

                        tick_texts = format_seconds_to_readable(pd.Series(tick_values), time_format_option).tolist()
                        fig_duration_hist.update_xaxes(tickvals=tick_values, ticktext=tick_texts)
                            
                st.plotly_chart(fig_duration_hist, use_container_width=True)
            else:
                st.info("No refresh duration data to display for histogram.", icon="ℹ️")
        else:
            st.info("No REFRESH_DURATION_SEC data available.", icon="ℹ️")

    chart_cols_state_row6 = st.columns([1]) # This ensures Row Change Overview gets its own row.

    with chart_cols_state_row6[0]: # Row Change Overview (Grouped Bar Chart)
        cols_chart6_header = st.columns([0.8, 0.2])
        with cols_chart6_header[0]:
            st.markdown("<p style='font-size:16px;'><b>Row Change Overview</b></p>", unsafe_allow_html=True)
        with cols_chart6_header[1]:
            use_log_scale_row_change = st.checkbox("Log Scale Y-axis", key="log_scale_row_change_dt_state")
        st.write("Total inserted, deleted, and copied rows per refresh action.")

        row_change_analysis_cols = ['NUMINSERTEDROWS', 'NUMDELETEDROWS', 'NUMCOPIEDROWS', 'NUMADDEDPARTITIONS', 'NUMREMOVEDPARTITIONS']
        
        available_row_analysis_cols = [col for col in row_change_analysis_cols if col in filtered_history_df.columns]

        if available_row_analysis_cols and not filtered_history_df.empty:
            row_change_summary = filtered_history_df.groupby('REFRESH_ACTION')[available_row_analysis_cols].sum().reset_index()

            if not row_change_summary.empty:
                row_change_melted = row_change_summary.melt(
                    id_vars='REFRESH_ACTION',
                    value_vars=available_row_analysis_cols,
                    var_name='Row_Metric',
                    value_name='Count'
                )
                fig_row_change = px.bar(
                    row_change_melted,
                    x='REFRESH_ACTION',
                    y='Count',
                    color='Row_Metric',
                    title='Row Changes by Refresh Action', # Redundant title
                    barmode='group',
                    labels={'REFRESH_ACTION': 'Refresh Action', 'Count': 'Number of Rows'}
                )
                fig_row_change.update_layout(title_text='') # Remove title_text

                if use_log_scale_row_change:
                    fig_row_change.update_yaxes(type='log', rangemode='tozero')
                    st.info("Logarithmic scale applied to Y-axis.", icon="ℹ️")

                st.plotly_chart(fig_row_change, use_container_width=True)
            else:
                st.info("No row change data after aggregation.", icon="ℹ️")
        else:
            st.info("Row change statistics (NUMINSERTEDROWS, etc.) not available or no data after filters.", icon="ℹ️")
    
    st.divider()

    # --- Failure & Anomaly Detection ---
    st.subheader("Failure & Anomaly Detection")

    # Chart 7: Daily Failure Count Trend
    cols_chart7_header = st.columns([0.8, 0.2])
    with cols_chart7_header[0]:
        st.markdown("<p style='font-size:16px;'><b>Daily Failure Count Trend</b></p>", unsafe_allow_html=True)
    with cols_chart7_header[1]:
        use_log_scale_failure_trend = st.checkbox("Log Scale Y-axis", key="log_scale_failure_trend_dt_state")
    st.write("Count of failed/cancelled refreshes over time.")
    
    chart_cols_state_row4 = st.columns([1, 1])

    with chart_cols_state_row4[0]:
        failed_trend_df = filtered_history_df[
            filtered_history_df['STATE'].isin(['FAILED', 'CANCELLED', 'UPSTREAM_FAILED'])
        ].copy()
        if not failed_trend_df.empty:
            failure_counts_daily = failed_trend_df.groupby(
                pd.Grouper(key='DATA_TIMESTAMP_DT', freq='D')
            ).size().reset_index(name='Failure Count')
            failure_counts_daily.rename(columns={'DATA_TIMESTAMP_DT': 'Date'}, inplace=True)

            fig_failure_trend = px.line(
                failure_counts_daily,
                x='Date',
                y='Failure Count',
                title='Daily Failed/Cancelled Refreshes', # Redundant title
                labels={'Failure Count': 'Number of Failures'}
            )
            fig_failure_trend.update_layout(title_text='') # Remove title_text

            if use_log_scale_failure_trend:
                fig_failure_trend.update_yaxes(type='log', rangemode='tozero')
                st.info("Logarithmic scale applied to Y-axis.", icon="ℹ️")

            st.plotly_chart(fig_failure_trend, use_container_width=True)
        else:
            st.info("No failed or cancelled refreshes to trend.", icon="ℹ️")

    with chart_cols_state_row4[1]: # Refresh Trigger Distribution
        st.markdown("<p style='font-size:16px;'><b>Refresh Trigger Distribution</b></p>", unsafe_allow_html=True)
        st.write("Distribution of refresh triggers (e.g., SCHEDULED, MANUAL).")
        if 'REFRESH_TRIGGER' in filtered_history_df.columns and not filtered_history_df.empty:
            trigger_counts = filtered_history_df['REFRESH_TRIGGER'].value_counts().reset_index()
            trigger_counts.columns = ['Trigger', 'Count']
            fig_trigger_dist = px.pie(
                trigger_counts,
                values='Count',
                names='Trigger',
                title='Refresh Trigger Distribution',
                hole=0.3
            )
            fig_trigger_dist.update_traces(textposition='inside', textinfo='percent+label')
            st.plotly_chart(fig_trigger_dist, use_container_width=True)
        else:
            st.info("No REFRESH_TRIGGER data available.", icon="ℹ️")
    
    st.divider()

    # --- Detailed Failed Refreshes Table ---
    st.subheader("Failed & Cancelled Refreshes Details")
    st.write("Detailed list of dynamic table refreshes that did not succeed.")

    failed_refreshes_df = filtered_history_df[
        filtered_history_df['STATE'].isin(['FAILED', 'CANCELLED', 'UPSTREAM_FAILED'])
    ].copy()

    if not failed_refreshes_df.empty:
        # Checkbox for "Show latest per table"
        show_latest_failed_per_table = st.checkbox("Show only the latest failed/cancelled refresh per table", key="latest_failed_per_table")

        if show_latest_failed_per_table:
            # Sort by QUALIFIED_NAME and then by DATA_TIMESTAMP_DT (descending)
            # Then group by QUALIFIED_NAME and take the first (most recent)
            failed_refreshes_to_display = failed_refreshes_df.sort_values(
                by=['QUALIFIED_NAME', 'DATA_TIMESTAMP_DT'], ascending=[True, False]
            ).groupby('QUALIFIED_NAME').first().reset_index()
            st.info("Displaying the latest failed/cancelled refresh for each affected table.", icon="ℹ️")
        else:
            failed_refreshes_to_display = failed_refreshes_df.sort_values('DATA_TIMESTAMP_DT', ascending=False)
        

        display_cols = [
            'QUALIFIED_NAME', 'STATE', 'STATE_CODE', 'STATE_MESSAGE', 'QUERY_ID',
            'DATA_TIMESTAMP_DT', 'REFRESH_START_TIME_DT', 'REFRESH_END_TIME_DT',
            'REFRESH_DURATION_SEC', 'REFRESH_ACTION', 'REFRESH_TRIGGER'
        ]
        
        if 'REFRESH_DURATION_SEC' in failed_refreshes_to_display.columns:
            failed_refreshes_to_display['REFRESH_DURATION_FMT'] = format_seconds_to_readable(
                failed_refreshes_to_display['REFRESH_DURATION_SEC'], time_format_option
            )
            display_cols = [col if col != 'REFRESH_DURATION_SEC' else 'REFRESH_DURATION_FMT' for col in display_cols]


        display_labels = {
            'QUALIFIED_NAME': 'Dynamic Table',
            'STATE': 'State',
            'STATE_CODE': 'Code',
            'STATE_MESSAGE': 'Message',
            'QUERY_ID': 'Query ID',
            'DATA_TIMESTAMP_DT': 'Data Timestamp',
            'REFRESH_START_TIME_DT': 'Refresh Start',
            'REFRESH_END_TIME_DT': 'Refresh End',
            'REFRESH_DURATION_FMT': 'Duration',
            'REFRESH_ACTION': 'Action',
            'REFRESH_TRIGGER': 'Trigger'
        }

        final_display_df = failed_refreshes_to_display[
            [col for col in display_cols if col in failed_refreshes_to_display.columns]
        ].rename(columns=display_labels)

        st.dataframe(final_display_df, use_container_width=True)
    else:
        st.info("No failed or cancelled refreshes found based on current filters. Great news!", icon="🎉")

    st.divider()

    # --- Top N Longest Refreshes Table ---
    st.subheader("Top Longest Refreshes")
    st.write("Identifies historical refresh events with the longest durations.")

    longest_refreshes_df = filtered_history_df[
        (filtered_history_df['STATE'] == 'SUCCEEDED') &
        (filtered_history_df['REFRESH_DURATION_SEC'].notna())
    ].copy()

    if not longest_refreshes_df.empty:
        # Checkbox for "Show longest per table"
        show_longest_per_table = st.checkbox("Show only the longest refresh per table", key="longest_refresh_per_table")

        if show_longest_per_table:
            longest_refreshes_to_display = longest_refreshes_df.sort_values(
                by=['QUALIFIED_NAME', 'REFRESH_DURATION_SEC'], ascending=[True, False]
            ).groupby('QUALIFIED_NAME').first().reset_index()
            st.info("Displaying the longest refresh for each successful table.", icon="ℹ️")
        else:
            longest_refreshes_to_display = longest_refreshes_df.sort_values('REFRESH_DURATION_SEC', ascending=False).head(10)
            
        
        longest_refreshes_to_display['REFRESH_DURATION_FMT'] = format_seconds_to_readable(
            longest_refreshes_to_display['REFRESH_DURATION_SEC'], time_format_option
        )

        display_cols_longest = [
            'QUALIFIED_NAME', 'REFRESH_DURATION_FMT', 'REFRESH_ACTION', 'REFRESH_TRIGGER',
            'REFRESH_START_TIME_DT', 'REFRESH_END_TIME_DT',
            'NUMINSERTEDROWS', 'NUMDELETEDROWS', 'NUMCOPIEDROWS', 'NUMADDEDPARTITIONS', 'NUMREMOVEDPARTITIONS'
        ]

        display_labels_longest = {
            'QUALIFIED_NAME': 'Dynamic Table',
            'REFRESH_DURATION_FMT': 'Duration',
            'REFRESH_ACTION': 'Action',
            'REFRESH_TRIGGER': 'Trigger',
            'REFRESH_START_TIME_DT': 'Start Time',
            'REFRESH_END_TIME_DT': 'End Time',
            'NUMINSERTEDROWS': 'Ins. Rows',
            'NUMDELETEDROWS': 'Del. Rows',
            'NUMCOPIEDROWS': 'Copied Rows',
            'NUMADDEDPARTITIONS': 'Added Part.',
            'NUMREMOVEDPARTITIONS': 'Rem. Part.'
        }

        final_display_longest_df = longest_refreshes_to_display[
            [col for col in display_cols_longest if col in longest_refreshes_to_display.columns]
        ].rename(columns=display_labels_longest)

        st.dataframe(final_display_longest_df, use_container_width=True)
    else:
        st.info("No successful refreshes found to identify longest durations.", icon="ℹ️")
    
    st.divider()

    # --- Top N Frequent Failure Messages / Codes Table ---
    st.subheader("Frequent Failure Patterns")
    st.write("Identifies recurring error messages or codes from failed refreshes.")

    failure_patterns_df = filtered_history_df[
        filtered_history_df['STATE'].isin(['FAILED', 'CANCELLED', 'UPSTREAM_FAILED'])
    ].copy()

    if not failure_patterns_df.empty:
        # Group by STATE_CODE and STATE_MESSAGE, count occurrences, and get the last occurrence time
        failure_summary = failure_patterns_df.groupby(['STATE_CODE', 'STATE_MESSAGE']).agg(
            Occurrence_Count=('STATE_CODE', 'size'),
            Last_Occurred_Time=('DATA_TIMESTAMP_DT', 'max')
        ).reset_index()
        
        failure_summary = failure_summary.sort_values('Occurrence_Count', ascending=False)

        display_cols_failure = ['STATE_CODE', 'STATE_MESSAGE', 'Occurrence_Count', 'Last_Occurred_Time']
        display_labels_failure = {
            'STATE_CODE': 'Error Code',
            'STATE_MESSAGE': 'Error Message',
            'Occurrence_Count': 'Count',
            'Last_Occurred_Time': 'Last Occurred'
        }

        final_display_failure_df = failure_summary[
            [col for col in display_cols_failure if col in failure_summary.columns]
        ].rename(columns=display_labels_failure)

        st.dataframe(final_display_failure_df, use_container_width=True)
    else:
        st.info("No failed or cancelled refreshes to analyze for frequent patterns.", icon="ℹ️")
