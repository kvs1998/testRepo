-- CREATE TABLE STATEMENT for T_DYNAMIC_TABLE_REFRESH_HISTORY_FLAT
-- Run this ONCE in your Snowflake environment before running the stored procedure.
-- Replace YOUR_DB and YOUR_SCHEMA with your actual database and schema names.

CREATE TABLE YOUR_DB.YOUR_SCHEMA.T_DYNAMIC_TABLE_REFRESH_HISTORY_FLAT (
    -- Metadata about the collection itself
    COLLECTION_TIMESTAMP TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP(), -- When this record was captured by the procedure

    -- Core Dynamic Table Identifiers
    DATABASE_NAME TEXT,
    SCHEMA_NAME TEXT,
    TABLE_NAME TEXT,
    QUALIFIED_NAME TEXT, -- Fully qualified name of the dynamic table

    -- Refresh State and Details
    STATE TEXT, -- Status of the refresh (SUCCEEDED, FAILED, EXECUTING, etc.)
    STATE_CODE TEXT,
    STATE_MESSAGE TEXT,
    QUERY_ID TEXT, -- Unique ID of the refresh SQL statement

    -- Timestamps
    DATA_TIMESTAMP TIMESTAMP_NTZ,      -- Transactional timestamp when the refresh data was evaluated
    REFRESH_START_TIME TIMESTAMP_NTZ,  -- Time when the refresh job started
    REFRESH_END_TIME TIMESTAMP_NTZ,    -- Time when the refresh completed (NULL if still executing)
    COMPLETION_TARGET TIMESTAMP_NTZ,   -- Time by which refresh should complete to meet TARGET_LAG

    -- Dependency Details (extracted from LAST_COMPLETED_DEPENDENCY OBJECT)
    LAST_COMPLETED_DEPENDENCY_QUALIFIED_NAME TEXT,
    LAST_COMPLETED_DEPENDENCY_DATA_TIMESTAMP TIMESTAMP_NTZ,

    -- Statistics (extracted from STATISTICS OBJECT)
    NUMINSERTEDROWS NUMBER,
    NUMDELETEDROWS NUMBER,
    NUMCOPIEDROWS NUMBER,
    NUMADDEDPARTITIONS NUMBER,
    NUMREMOVEDPARTITIONS NUMBER,

    -- Refresh Action & Trigger
    REFRESH_ACTION TEXT, -- Why the refresh happened (INCREMENTAL, FULL, NO_DATA, REINITIALIZE)
    REFRESH_TRIGGER TEXT, -- How the refresh was initiated (SCHEDULED, MANUAL, CREATION)
    
    -- Lag Metrics
    TARGET_LAG_SEC NUMBER, -- Target lag value for the dynamic table at time of refresh

    -- Graph History Reference
    GRAPH_HISTORY_VALID_FROM TIMESTAMP_NTZ, -- Timestamp reference for DYNAMIC_TABLE_GRAPH_HISTORY


    -- Primary Key for Deduplication and Uniqueness
    -- This composite key uniquely identifies a specific refresh event at a given collection point.
    -- (QUALIFIED_NAME, DATA_TIMESTAMP, REFRESH_START_TIME) is a good logical primary key
    -- for a refresh event. Adding QUERY_ID makes it even more robust if available.
    -- COLLECTION_TIMESTAMP is crucial because you might collect multiple times and want
    -- to know which snapshot recorded which state transition if updating.
    PRIMARY KEY (COLLECTION_TIMESTAMP, QUERY_ID, QUALIFIED_NAME, DATA_TIMESTAMP, REFRESH_START_TIME)
);

-- Optional: Add clustering keys for query performance if your dashboard frequently filters/sorts by these columns
-- Example: CLUSTER BY (QUALIFIED_NAME, DATA_TIMESTAMP);





CREATE OR REPLACE PROCEDURE YOUR_DB.YOUR_SCHEMA.SP_COLLECT_DT_REFRESH_HISTORY(
    P_DATABASE_NAME VARCHAR,
    P_SCHEMA_NAME VARCHAR,
    P_TABLE_NAME VARCHAR,
    P_COLLECT_FROM_LAST_DAYS NUMBER DEFAULT 7
)
RETURNS VARCHAR
LANGUAGE SQL
EXECUTE AS CALLER
AS
$$
DECLARE
    lv_func_args VARCHAR := '';
    lv_merge_action_desc VARCHAR;
    lv_start_timestamp TIMESTAMP_LTZ;
    lv_end_timestamp TIMESTAMP_LTZ := CURRENT_TIMESTAMP();
    lv_error_message VARCHAR;
    lv_filter_provided BOOLEAN := FALSE; -- Flag to track if any specific filter was provided
BEGIN
    -- Step 1: Validate input parameters for specificity
    IF (:P_DATABASE_NAME IS NULL AND :P_SCHEMA_NAME IS NULL AND :P_TABLE_NAME IS NULL) THEN
        RETURN 'ERROR: At least one of P_DATABASE_NAME, P_SCHEMA_NAME, or P_TABLE_NAME must be provided (cannot be NULL) for collection.';
    END IF;

    -- Step 2: Calculate start timestamp
    lv_start_timestamp := DATEADD(day, -:P_COLLECT_FROM_LAST_DAYS, :lv_end_timestamp);

    -- Step 3: Construct arguments for DYNAMIC_TABLE_REFRESH_HISTORY table function
    -- This section aims to build the most specific filter possible
    IF (:P_DATABASE_NAME IS NOT NULL AND :P_SCHEMA_NAME IS NOT NULL AND :P_TABLE_NAME IS NOT NULL) THEN
        lv_func_args := 'NAME => ''' || :P_DATABASE_NAME || '.' || :P_SCHEMA_NAME || '.' || :P_TABLE_NAME || '''';
        lv_filter_provided := TRUE;
    ELSEIF (:P_SCHEMA_NAME IS NOT NULL AND :P_TABLE_NAME IS NOT NULL) THEN
        lv_func_args := 'NAME => ''' || :P_SCHEMA_NAME || '.' || :P_TABLE_NAME || '''';
        lv_filter_provided := TRUE;
    ELSEIF (:P_TABLE_NAME IS NOT NULL) THEN
        lv_func_args := 'NAME => ''' || :P_TABLE_NAME || '''';
        lv_filter_provided := TRUE;
    ELSEIF (:P_DATABASE_NAME IS NOT NULL AND :P_SCHEMA_NAME IS NOT NULL) THEN
        lv_func_args := 'NAME_PREFIX => ''' || :P_DATABASE_NAME || '.' || :P_SCHEMA_NAME || '''';
        lv_filter_provided := TRUE;
    ELSEIF (:P_DATABASE_NAME IS NOT NULL) THEN
        lv_func_args := 'NAME_PREFIX => ''' || :P_DATABASE_NAME || '''';
        lv_filter_provided := TRUE;
    ELSEIF (:P_SCHEMA_NAME IS NOT NULL) THEN
        -- Note: Filtering by SCHEMA_NAME alone with NAME_PREFIX might bring tables from other DBs
        -- with the same schema name. This is an accepted behavior per the function definition.
        lv_func_args := 'NAME_PREFIX => ''' || :P_SCHEMA_NAME || '''';
        lv_filter_provided := TRUE;
    END IF;

    -- If no specific name filter could be constructed (e.g., only RESULT_LIMIT, DATA_TIMESTAMP_START are implicitly passed),
    -- and we already returned for (NULL,NULL,NULL), this check ensures lv_func_args is indeed populated if a specific parameter was given.
    IF (NOT lv_filter_provided) THEN
         RETURN 'ERROR: Invalid combination of NULL parameters provided. Please provide a specific DB, Schema, or Table name for collection.';
    END IF;


    -- Add timestamp range and high RESULT_LIMIT
    IF (lv_func_args != '') THEN
        lv_func_args := lv_func_args || ', ';
    END IF;
    lv_func_args := lv_func_args || 'DATA_TIMESTAMP_START => ''' || :lv_start_timestamp || '''::TIMESTAMP_LTZ, ';
    lv_func_args := lv_func_args || 'DATA_TIMESTAMP_END => ''' || :lv_end_timestamp || '''::TIMESTAMP_LTZ, ';
    lv_func_args := lv_func_args || 'RESULT_LIMIT => 100000'; -- High limit

    -- Step 4: Execute MERGE INTO statement
    EXECUTE IMMEDIATE '
        MERGE INTO YOUR_DB.YOUR_SCHEMA.T_DYNAMIC_TABLE_REFRESH_HISTORY_FLAT AS target_table
        USING (
            SELECT
                CURRENT_TIMESTAMP()::TIMESTAMP_NTZ AS COLLECTION_TIMESTAMP_SOURCE,
                dtrh.DATABASE_NAME,
                dtrh.SCHEMA_NAME,
                dtrh.NAME AS TABLE_NAME,
                dtrh.QUALIFIED_NAME,
                dtrh.STATE,
                dtrh.STATE_CODE,
                dtrh.STATE_MESSAGE,
                dtrh.QUERY_ID,
                dtrh.DATA_TIMESTAMP::TIMESTAMP_NTZ AS DATA_TIMESTAMP,
                dtrh.REFRESH_START_TIME::TIMESTAMP_NTZ AS REFRESH_START_TIME,
                dtrh.REFRESH_END_TIME::TIMESTAMP_NTZ AS REFRESH_END_TIME,
                dtrh.COMPLETION_TARGET::TIMESTAMP_NTZ AS COMPLETION_TARGET,
                dep.value:qualified_name::TEXT AS LAST_COMPLETED_DEPENDENCY_QUALIFIED_NAME,
                dep.value:data_timestamp::TIMESTAMP_NTZ AS LAST_COMPLETED_DEPENDENCY_DATA_TIMESTAMP,
                stats.value:numInsertedRows::NUMBER AS NUMINSERTEDROWS,
                stats.value:numDeletedRows::NUMBER AS NUMDELETEDROWS,
                stats.value:numCopiedRows::NUMBER AS NUMCOPIEDROWS,
                stats.value:numAddedPartitions::NUMBER AS NUMADDEDPARTITIONS,
                stats.value:numRemovedPartitions::NUMBER AS NUMREMOVEDPARTITIONS,
                dtrh.REFRESH_ACTION,
                dtrh.REFRESH_TRIGGER,
                dtrh.TARGET_LAG_SEC,
                dtrh.GRAPH_HISTORY_VALID_FROM::TIMESTAMP_NTZ AS GRAPH_HISTORY_VALID_FROM
            FROM
                TABLE(INFORMATION_SCHEMA.DYNAMIC_TABLE_REFRESH_HISTORY(' || lv_func_args || ')) dtrh,
                LATERAL FLATTEN(INPUT => dtrh.LAST_COMPLETED_DEPENDENCY, OUTER => TRUE) dep,
                LATERAL FLATTEN(INPUT => dtrh.STATISTICS, OUTER => TRUE) stats
        ) AS source_data
        ON target_table.QUERY_ID = source_data.QUERY_ID
        AND target_table.QUALIFIED_NAME = source_data.QUALIFIED_NAME
        AND target_table.DATA_TIMESTAMP = source_data.DATA_TIMESTAMP
        AND target_table.REFRESH_START_TIME = source_data.REFRESH_START_TIME

        WHEN MATCHED AND target_table.STATE != source_data.STATE THEN
            UPDATE SET
                target_table.COLLECTION_TIMESTAMP = source_data.COLLECTION_TIMESTAMP_SOURCE,
                target_table.STATE = source_data.STATE,
                target_table.STATE_CODE = source_data.STATE_CODE,
                target_table.STATE_MESSAGE = source_data.STATE_MESSAGE,
                target_table.REFRESH_END_TIME = source_data.REFRESH_END_TIME,
                target_table.NUMINSERTEDROWS = source_data.NUMINSERTEDROWS,
                target_table.NUMDELETEDROWS = source_data.NUMDELETEDROWS,
                target_table.NUMCOPIEDROWS = source_data.NUMCOPIEDROWS,
                target_table.NUMADDEDPARTITIONS = source_data.NUMADDEDPARTITIONS,
                target_table.NUMREMOVEDPARTITIONS = source_data.NUMREMOVEDPARTITIONS,
                target_table.REFRESH_ACTION = source_data.REFRESH_ACTION,
                target_table.REFRESH_TRIGGER = source_data.REFRESH_TRIGGER,
                target_table.TARGET_LAG_SEC = source_data.TARGET_LAG_SEC,
                target_table.GRAPH_HISTORY_VALID_FROM = source_data.GRAPH_HISTORY_VALID_FROM
        WHEN NOT MATCHED THEN
            INSERT (
                COLLECTION_TIMESTAMP,
                DATABASE_NAME, SCHEMA_NAME, TABLE_NAME, QUALIFIED_NAME, STATE, STATE_CODE, STATE_MESSAGE, QUERY_ID,
                DATA_TIMESTAMP, REFRESH_START_TIME, REFRESH_END_TIME, COMPLETION_TARGET,
                LAST_COMPLETED_DEPENDENCY_QUALIFIED_NAME, LAST_COMPLETED_DEPENDENCY_DATA_TIMESTAMP,
                NUMINSERTEDROWS, NUMDELETEDROWS, NUMCOPIEDROWS, NUMADDEDPARTITIONS, NUMREMOVEDPARTITIONS,
                REFRESH_ACTION, REFRESH_TRIGGER, TARGET_LAG_SEC, GRAPH_HISTORY_VALID_FROM
            )
            VALUES (
                source_data.COLLECTION_TIMESTAMP_SOURCE,
                source_data.DATABASE_NAME, source_data.SCHEMA_NAME, source_data.TABLE_NAME, source_data.QUALIFIED_NAME, source_data.STATE, source_data.STATE_CODE, source_data.STATE_MESSAGE, source_data.QUERY_ID,
                source_data.DATA_TIMESTAMP, source_data.REFRESH_START_TIME, source_data.REFRESH_END_TIME, source_data.COMPLETION_TARGET,
                source_data.LAST_COMPLETED_DEPENDENCY_QUALIFIED_NAME, source_data.LAST_COMPLETED_DEPENDENCY_DATA_TIMESTAMP,
                source_data.NUMINSERTEDROWS, source_data.NUMDELETEDROWS, source_data.NUMCOPIEDROWS, source_data.NUMADDEDPARTITIONS, source_data.NUMREMOVEDPARTITIONS,
                source_data.REFRESH_ACTION, source_data.REFRESH_TRIGGER, source_data.TARGET_LAG_SEC, source_data.GRAPH_HISTORY_VALID_FROM
            );
    ';

    -- Get the merge statistics (optional, but good for auditing)
    SELECT
        "number of rows inserted" || ' inserted, ' ||
        "number of rows updated" || ' updated.'
    INTO lv_merge_action_desc
    FROM TABLE(RESULT_SCAN(LAST_QUERY_ID()));

    RETURN 'Successfully merged ' || lv_merge_action_desc || ' into T_DYNAMIC_TABLE_REFRESH_HISTORY_FLAT for ' || COALESCE(:P_DATABASE_NAME, 'ALL_DB') || '.' || COALESCE(:P_SCHEMA_NAME, 'ALL_SCHEMA') || '.' || COALESCE(:P_TABLE_NAME, 'ALL_TABLES') || ' in range ' || :lv_start_timestamp || ' to ' || :lv_end_timestamp || '.';

EXCEPTION
    WHEN OTHER THEN
        lv_error_message := 'Error during DTRH collection for ' || COALESCE(:P_DATABASE_NAME, 'N/A') || '.' || COALESCE(:P_SCHEMA_NAME, 'N/A') || '.' || COALESCE(:P_TABLE_NAME, 'N/A') || ' in range ' || :lv_start_timestamp || ' to ' || :lv_end_timestamp || '. SQLSTATE: ' || SQLSTATE() || ', SQLCODE: ' || SQLCODE() || ', Message: ' || SQLERRM();
        RETURN lv_error_message;
END;
$$;
